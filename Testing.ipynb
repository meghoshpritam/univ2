{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6DWt3G5knI-"
      },
      "source": [
        "# Appriori\n",
        "import numpy as np\n",
        "from python_utils import *\n",
        "from itertools import combinations, chain\n",
        "\n",
        "def count_occurences(itemset, Transactions):\n",
        "  count = 0\n",
        "  for i in range(len(Transactions)):\n",
        "    if set(itemset).issubset(set(Transactions[i])):\n",
        "      count += 1\n",
        "  return count\n",
        "def join_two_itemsets(it1, it2, order):\n",
        "  it1.sort(key = lambda x: order.index(x))\n",
        "  it2.sort(key = lambda x: order.index(x))\n",
        "  for i in range(len(it1) - 1):\n",
        "    if it1[i] != it2[i]:\n",
        "      return []\n",
        "  if order.index(it1[-1]) < order.index(it2[-1]):\n",
        "    return it1 + [it2[-1]]\n",
        "\n",
        "  return []\n",
        "\n",
        "def join_set_itemsets(set_of_its, order):\n",
        "  C = []\n",
        "  for i in range(len(set_of_its)):\n",
        "    for j in range(i+1, len(set_of_its)):\n",
        "      it_out = join_two_itemsets(set_of_its[i], set_of_its[j], order)\n",
        "      if len(it_out) > 0:\n",
        "        C.append(it_out)\n",
        "  return C\n",
        "\n",
        "def get_frequent(itemsets, Transactions, min_support, prev_discarded):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  \n",
        "  k = len(prev_discarded.keys())\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    discarded_before = False\n",
        "    if k > 0:\n",
        "      for it in prev_discarded[k]:\n",
        "        if set(it).issubset(set(itemsets[s])):\n",
        "          discarded_before = True\n",
        "          break\n",
        "\n",
        "    if not discarded_before:\n",
        "      count = count_occurences(itemsets[s], Transactions)\n",
        "      if count/len(Transactions) >= min_support:\n",
        "        L.append(itemsets[s])\n",
        "        supp_count.append(count)\n",
        "      else:\n",
        "        new_discarded.append(itemsets[s])\n",
        "  return L , supp_count, new_discarded\n",
        "  \n",
        "def load_transactions(path_to_data, order):\n",
        "  Transactions = []\n",
        "  with open(path_to_data, 'r') as fid:\n",
        "    for lines in fid:\n",
        "      str_line = list(lines.strip().split(','))\n",
        "      _t = list(np.unique(str_line))\n",
        "      _t.sort(key = lambda x: order.index(x))\n",
        "      Transactions.append(_t)\n",
        "  return Transactions\n",
        "\n",
        "def print_table(T, supp_count):\n",
        "  print(\"Itemset | Frequency\")\n",
        "  for k in range(len(T)):\n",
        "    print(\"{} : {}\".format(T[k], supp_count[k]))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "import csv\n",
        "Transactions = []\n",
        "with open('/content/drive/MyDrive/Pamir/tdb.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "for i in range(len(data)):\n",
        "  Transactions.append(data[i])\n",
        "\n",
        "\n",
        "num_trans = len(Transactions)\n",
        "# Transactions\n",
        "# Transactions\n",
        "orders = []\n",
        "orders.append(Transactions[0][0])\n",
        "for i in range(0,len(Transactions)):\n",
        "  for j in range(len(Transactions[i])):\n",
        "    count = 0\n",
        "    k = 0\n",
        "    while(k < len(orders)):\n",
        "      if Transactions[i][j] == orders[k]:\n",
        "        count += 1\n",
        "        break\n",
        "      k += 1\n",
        "    if count == 0:\n",
        "      orders.append(Transactions[i][j])\n",
        "\n",
        "order = sorted(orders)\n",
        "\n",
        "C = {}\n",
        "L = {}\n",
        "itemset_size = 1\n",
        "Discarded = { itemset_size : []}\n",
        "C.update({itemset_size : [ [f] for f in order]})\n",
        "\n",
        "min_support = 1/5\n",
        "min_confidence = 1\n",
        "supp_count_L = {}\n",
        "f, sup, new_discarded = get_frequent(C[itemset_size], Transactions, min_support, Discarded)\n",
        "Discarded.update({itemset_size : new_discarded})\n",
        "L.update({itemset_size : f})\n",
        "supp_count_L.update({itemset_size : sup})\n",
        "\n",
        "print_table(L[1], supp_count_L[1])\n",
        "k = itemset_size + 1\n",
        "convergence = False\n",
        "while not convergence:\n",
        "  C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "  print(\"Table C{}:\\n\".format(k))\n",
        "  print_table(C[k], [count_occurences(it, Transactions) for it in C[k]])\n",
        "  f,sup, new_discarded = get_frequent(C[k], Transactions, min_support,Discarded)\n",
        "  Discarded.update({k : new_discarded})\n",
        "  L.update({k : f})\n",
        "  supp_count_L.update({k : sup})\n",
        "  if len(L[k]) == 0:\n",
        "    convergence = True\n",
        "  else:\n",
        "    print(\"Table L{}:\\n\".format(k))\n",
        "    print_table(L[k], supp_count_L[k])\n",
        "  k += 1\n",
        "\n",
        "def powerset(s):\n",
        "  return list(chain.from_iterable(combinations(s,r) for r in range(1,len(s) + i)))\n",
        "\n",
        "def write_rules(X, X_S, S, conf, supp, num_trans):\n",
        "  out_rules = \"\"\n",
        "  out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "  out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "  out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "  out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  \n",
        "  return out_rules\n",
        "\n",
        "\n",
        "assoc_rules_str = \"\"\n",
        "for i in range(1, len(L)):\n",
        "  for j in range(len(L[i])):\n",
        "    s = powerset(L[i][j])\n",
        "    s.pop()\n",
        "    for z in s:\n",
        "      S = set(z)\n",
        "      X = set(L[i][j])\n",
        "      X_S = set(X-S)\n",
        "      sup_x = count_occurences(X , Transactions)\n",
        "      sup_x_s = count_occurences(X_S, Transactions)\n",
        "      conf = sup_x / count_occurences(S, Transactions)\n",
        "      if conf >= min_confidence and sup_x >= min_support:\n",
        "        assoc_rules_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbVJSEETk-YA",
        "outputId": "72b7c635-f9d9-46b9-a4e8-c10aaa3c88ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xM8UpTplHnY",
        "outputId": "9c4d446c-6dc9-4799-d615-5a24b75407f1"
      },
      "source": [
        "order = ['I' + str(i) for i in range(1,6)]\n",
        "print(order)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I1', 'I2', 'I3', 'I4', 'I5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkOkiwJsnnvd",
        "outputId": "34e4dc2f-ada2-40d9-ca01-49ea96ff885d"
      },
      "source": [
        "import csv\n",
        "Transactions = []\n",
        "with open('/content/drive/MyDrive/Pamir/tdb.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "for i in range(len(data)):\n",
        "  Transactions.append(sorted(data[i]))\n",
        "\n",
        "\n",
        "orders = []\n",
        "orders.append(Transactions[0][0])\n",
        "for i in range(0,len(Transactions)):\n",
        "  for j in range(len(Transactions[i])):\n",
        "    count = 0\n",
        "    k = 0\n",
        "    while(k < len(orders)):\n",
        "      if Transactions[i][j] == orders[k]:\n",
        "        count += 1\n",
        "        break\n",
        "      k += 1\n",
        "    if count == 0:\n",
        "      orders.append(Transactions[i][j])\n",
        "\n",
        "orders = sorted(orders)\n",
        "print(len(orders))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x4PCXJhrqXv"
      },
      "source": [
        "import numpy as np\n",
        "from python_utils import *\n",
        "from itertools import combinations, chain\n",
        "\n",
        "def count_occurences(itemset, Transactions):\n",
        "  count = 0\n",
        "  for i in range(len(Transactions)):\n",
        "    if set(itemset).issubset(set(Transactions[i])):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def get_frequent(itemsets, Transactions):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  num_trans = len(Transactions)\n",
        "  # k = len(prev_discarded.keys())\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    count = count_occurences(itemsets[s], Transactions)\n",
        "    \n",
        "    L.append(itemsets[s])\n",
        "    supp_count.append(count)\n",
        "    # else:\n",
        "    #   new_discarded.append(itemsets[s])\n",
        "  return L , supp_count\n",
        "  \n",
        "def load_transactions(path_to_data, order):\n",
        "  Transactions = []\n",
        "  with open(path_to_data, 'r') as fid:\n",
        "    for lines in fid:\n",
        "      str_line = list(lines.strip().split(','))\n",
        "      _t = list(np.unique(str_line))\n",
        "      _t.sort(key = lambda x: order.index(x))\n",
        "      Transactions.append(_t)\n",
        "  return Transactions\n",
        "\n",
        "def print_PIS_table(T, supp_count, PIS):\n",
        "  print(\"Itemset|Frequency|PIS\")\n",
        "  for k in range(len(T)):\n",
        "    print(\"{} : {} : {}\".format(T[k], supp_count[k], PIS[k]))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_MIS_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "arr2 = []\n",
        "\n",
        "def print_Exact_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Exact Itemsets\")\n",
        "  print(\"-------------------------------------------\")\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  count = 0\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    if(supp_count[k] == min(arr)):\n",
        "      arr2.append(T[k])\n",
        "      print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "      count += 1\n",
        "  if(count ==  0):\n",
        "        print(\"There is no exact itemsets\")\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "def join_two_itemsets(it1, it2, order):\n",
        "  it1.sort(key = lambda x: order.index(x))\n",
        "  it2.sort(key = lambda x: order.index(x))\n",
        "\n",
        "  for i in range(len(it1) - 1):\n",
        "    if it1[i] != it2[i]:\n",
        "      return []\n",
        "  if order.index(it1[-1]) < order.index(it2[-1]):\n",
        "    return it1 + [it2[-1]]\n",
        "\n",
        "  return []\n",
        "\n",
        "def join_set_itemsets(set_of_its, order):\n",
        "  C = []\n",
        "  for i in range(len(set_of_its)):\n",
        "    for j in range(i+1, len(set_of_its)):\n",
        "      it_out = join_two_itemsets(set_of_its[i], set_of_its[j], order)\n",
        "      if len(it_out) > 0:\n",
        "        C.append(it_out)\n",
        "  return C\n",
        "\n",
        "path_to_data = \"/content/drive/MyDrive/Pamir/data.txt\"\n",
        "order = ['I' + str(i) for i in range(1,6)]\n",
        "Transactions = load_transactions(path_to_data,order)\n",
        "# Transactions\n",
        "C = {}\n",
        "L = {}\n",
        "min_conf = 1\n",
        "itemset_size = 1\n",
        "num_trans = len(Transactions)\n",
        "# Discarded = { itemset_size : []}\n",
        "C.update({itemset_size : [ [f] for f in order]})\n",
        "C\n",
        "\n",
        "supp_count_L = {}\n",
        "f, sup = get_frequent(C[itemset_size], Transactions)\n",
        "# Discarded.update({itemset_size : new_discarded})\n",
        "L.update({itemset_size : f})\n",
        "supp_count_L.update({itemset_size : sup})\n",
        "\n",
        "print(len(C[itemset_size]))\n",
        "PIS = []\n",
        "Dict = {}\n",
        "for i in range(len(C[itemset_size])):\n",
        "  n = int(input(\"Enter item PIS: \"))\n",
        "  PIS.append(n)\n",
        "\n",
        "# List is converting into string  \n",
        "def convertList(list1):  \n",
        "  \n",
        "    return ' '.join([str(e) for e in list1]) #List comprehension  \n",
        "\n",
        "for i in range(len(L[1])):\n",
        "    # case = {L[1][i] : supp_count_L[1][i]}\n",
        "    case = {convertList(L[1][i]) : supp_count_L[1][i]}\n",
        "    Dict.update(case)\n",
        "# print(Dict)\n",
        "print_PIS_table(L[1], supp_count_L[1], PIS)\n",
        "\n",
        "k = itemset_size + 1\n",
        "convergence = False\n",
        "while not convergence:\n",
        "  arr = []\n",
        "  C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "  print(\"Table C{}:\\n\".format(k))\n",
        "  print_MIS_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "  print_Exact_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "  f,sup = get_frequent(C[k], Transactions)\n",
        "  L.update({k : f})\n",
        "  \n",
        "  supp_count_L.update({k : sup})\n",
        "  if len(L[k]) == 1:\n",
        "    convergence = True\n",
        "  else:\n",
        "    print()\n",
        "  k += 1\n",
        "\n",
        "def powerset(s):\n",
        "  return list(chain.from_iterable(combinations(s,r) for r in range(1,len(s) + i)))\n",
        "\n",
        "def write_rules(X, X_S, S, conf, supp, num_trans):\n",
        "  out_rules = \"\"\n",
        "  out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "  out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "  out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "  # out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  \n",
        "  return out_rules\n",
        "assoc_rules_str = \"\"\n",
        "for i in range(len(arr2)):\n",
        "  # for j in range(len(arr2[i])):\n",
        "  s = list(powerset(arr2[i]))\n",
        "  s.pop()\n",
        "  for z in s:\n",
        "    S = set(z)\n",
        "    X = set(arr2[i])\n",
        "    X_S = set(X-S)\n",
        "    sup_x = count_occurences(X , Transactions)\n",
        "    sup_x_s = count_occurences(X_S, Transactions)\n",
        "    conf = sup_x / count_occurences(S, Transactions)\n",
        "    if conf >= min_conf:\n",
        "      assoc_rules_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBDCJp_QeVSe"
      },
      "source": [
        "# Exact rule mining and apriori algorithm\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "from python_utils import *\n",
        "from itertools import combinations, chain\n",
        "\n",
        "def count_occurences(itemset, Transactions):\n",
        "  count = 0\n",
        "  for i in range(len(Transactions)):\n",
        "    if set(itemset).issubset(set(Transactions[i])):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def get_frequent_apriori(itemsets, Transactions, min_support, prev_discarded):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  \n",
        "  k = len(prev_discarded.keys())\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    discarded_before = False\n",
        "    if k > 0:\n",
        "      for it in prev_discarded[k]:\n",
        "        if set(it).issubset(set(itemsets[s])):\n",
        "          discarded_before = True\n",
        "          break\n",
        "\n",
        "    if not discarded_before:\n",
        "      count = count_occurences(itemsets[s], Transactions)\n",
        "      if count/len(Transactions) >= min_support:\n",
        "        L.append(itemsets[s])\n",
        "        supp_count.append(count)\n",
        "      else:\n",
        "        new_discarded.append(itemsets[s])\n",
        "  return L , supp_count, new_discarded\n",
        "\n",
        "def get_frequent_exact(itemsets, Transactions):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  num_trans = len(Transactions)\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    count = count_occurences(itemsets[s], Transactions)\n",
        "    \n",
        "    L.append(itemsets[s])\n",
        "    supp_count.append(count)\n",
        "  return L , supp_count\n",
        "\n",
        "def load_transactions(path_to_data, order):\n",
        "    Transactions = []\n",
        "    with open(path_to_data, 'r') as fid:\n",
        "      for lines in fid:\n",
        "        str_line = list(lines.strip().split(','))\n",
        "        _t = list(np.unique(str_line))\n",
        "        _t.sort(key = lambda x: order.index(x))\n",
        "        Transactions.append(_t)\n",
        "    return Transactions\n",
        "\n",
        "def print_PIS_table(T, supp_count, PIS):\n",
        "  print(\"Itemset|Frequency|PIS\")\n",
        "  for k in range(len(T)):\n",
        "    if(supp_count[k] <= PIS[k]):\n",
        "      print(\"{} : {} : {}\".format(T[k], supp_count[k], PIS[k]))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_MIS_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "arr2 = []\n",
        "\n",
        "def print_Exact_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Exact Itemsets\")\n",
        "  print(\"-------------------------------------------\")\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  count = 0\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    if(supp_count[k] == min(arr)):\n",
        "      arr2.append(T[k])\n",
        "      print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "      count += 1\n",
        "  if(count ==  0):\n",
        "        print(\"There is no exact itemsets\")\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_table(T, supp_count):\n",
        "    print(\"Itemset | Frequency\")\n",
        "    for k in range(len(T)):\n",
        "      print(\"{} : {}\".format(T[k], supp_count[k]))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "def join_two_itemsets(it1, it2, order):\n",
        "    it1.sort(key = lambda x: order.index(x))\n",
        "    it2.sort(key = lambda x: order.index(x))\n",
        "\n",
        "    for i in range(len(it1) - 1):\n",
        "      if it1[i] != it2[i]:\n",
        "        return []\n",
        "    if order.index(it1[-1]) < order.index(it2[-1]):\n",
        "      return it1 + [it2[-1]]\n",
        "\n",
        "    return []\n",
        "\n",
        "def join_set_itemsets(set_of_its, order):\n",
        "    C = []\n",
        "    for i in range(len(set_of_its)):\n",
        "      for j in range(i+1, len(set_of_its)):\n",
        "        it_out = join_two_itemsets(set_of_its[i], set_of_its[j], order)\n",
        "        if len(it_out) > 0:\n",
        "          C.append(it_out)\n",
        "    return C\n",
        "\n",
        "def powerset(s):\n",
        "  return list(chain.from_iterable(combinations(s,r) for r in range(1,len(s) + i)))\n",
        "\n",
        "\n",
        "def write_rules(X, X_S, S, conf, supp, num_trans):\n",
        "  out_rules = \"\"\n",
        "  out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "  out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "  out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "#   out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  \n",
        "  return out_rules\n",
        "\n",
        "Transactions = []\n",
        "# /content/drive/MyDrive/Pamir/tdb - Copy.csv\n",
        "with open('/content/drive/MyDrive/Pamir/tdb - Copy.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "for i in range(len(data)):\n",
        "  Transactions.append(data[i])\n",
        "\n",
        "num_trans = len(Transactions)\n",
        "# Transactions\n",
        "# Transactions\n",
        "orders = []\n",
        "orders.append(Transactions[0][0])\n",
        "for i in range(0,len(Transactions)):\n",
        "  for j in range(len(Transactions[i])):\n",
        "    count = 0\n",
        "    k = 0\n",
        "    while(k < len(orders)):\n",
        "      if Transactions[i][j] == orders[k]:\n",
        "        count += 1\n",
        "        break\n",
        "      k += 1\n",
        "    if count == 0:\n",
        "      orders.append(Transactions[i][j])\n",
        "\n",
        "order = sorted(orders)\n",
        "\n",
        "C = {}\n",
        "L = {}\n",
        "itemset_size = 1\n",
        "num_trans = len(Transactions)\n",
        "supp_count_L = {}\n",
        "C.update({itemset_size : [ [f] for f in order]})\n",
        "\n",
        "assoc_rules_apriori_str = \"\"\n",
        "assoc_rules_exact_str = \"\"\n",
        "def aprioriAlgo(assoc_rules_apriori_str):\n",
        "    min_support = 5/10\n",
        "    min_confidence = 1\n",
        "    Discarded = { itemset_size : []}\n",
        "    f, sup, new_discarded = get_frequent_apriori(C[itemset_size], Transactions, min_support, Discarded)\n",
        "    Discarded.update({itemset_size : new_discarded})\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})\n",
        "    print_table(L[1], supp_count_L[1])\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_table(C[k], [count_occurences(it, Transactions) for it in C[k]])\n",
        "      f,sup, new_discarded = get_frequent_apriori(C[k], Transactions, min_support,Discarded)\n",
        "      Discarded.update({k : new_discarded})\n",
        "      L.update({k : f})\n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 0:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print(\"Table L{}:\\n\".format(k))\n",
        "          print_table(L[k], supp_count_L[k])\n",
        "      k += 1\n",
        "\n",
        "    for i in range(1,len(L)):\n",
        "      for j in range(len(L[i])):\n",
        "        s = powerset(L[i][j])\n",
        "        s.pop()\n",
        "        for z in s:\n",
        "          S = set(z)\n",
        "          X = set(L[i][j])\n",
        "          X_S = set(X-S)\n",
        "          sup_x = count_occurences(X , Transactions)\n",
        "          sup_x_s = count_occurences(X_S, Transactions)\n",
        "          conf = sup_x / count_occurences(S, Transactions)\n",
        "          if conf >= min_confidence and sup_x >= min_support:\n",
        "              assoc_rules_apriori_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "\n",
        "    return assoc_rules_apriori_str\n",
        "\n",
        "def exactRule(assoc_rules_exact_str):\n",
        "    assoc_rules_str = \"\"\n",
        "    min_conf = 1\n",
        "    f, sup = get_frequent_exact(C[itemset_size], Transactions)\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})\n",
        "    PIS = []\n",
        "    Dict = {}\n",
        "    for i in range(len(C[itemset_size])):\n",
        "      n = int(input(\"Enter item PIS: \"))\n",
        "      PIS.append(n)\n",
        "\n",
        "    # List is converting into string  \n",
        "    def convertList(list1):  \n",
        "    \n",
        "        return ' '.join([str(e) for e in list1]) #List comprehension  \n",
        "\n",
        "    for i in range(len(L[1])):\n",
        "        # case = {L[1][i] : supp_count_L[1][i]}\n",
        "        case = {convertList(L[1][i]) : supp_count_L[1][i]}\n",
        "        Dict.update(case)\n",
        "    # print(Dict)\n",
        "    \n",
        "    print_PIS_table(L[1], supp_count_L[1], PIS)\n",
        "\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      arr = []\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_MIS_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      print_Exact_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      f,sup = get_frequent_exact(C[k], Transactions)\n",
        "      L.update({k : f})\n",
        "      \n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 1:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print()\n",
        "      k += 1\n",
        "\n",
        "    # for i in range(1, len(L)):\n",
        "    for i in range(len(arr2)):\n",
        "      # for j in range(len(arr2[i])):\n",
        "      s = list(powerset(arr2[i]))\n",
        "      s.pop()\n",
        "      for z in s:\n",
        "        S = set(z)\n",
        "        X = set(arr2[i])\n",
        "        X_S = set(X-S)\n",
        "        sup_x = count_occurences(X , Transactions)\n",
        "        sup_x_s = count_occurences(X_S, Transactions)\n",
        "        conf = sup_x / count_occurences(S, Transactions)\n",
        "        if conf >= min_conf:\n",
        "          assoc_rules_exact_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "          \n",
        "    return assoc_rules_exact_str\n",
        "print(\"***********************Rule mining using apriori algorithm***********************\")\n",
        "assoc_rules_apriori_str = aprioriAlgo(assoc_rules_apriori_str)\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_apriori_str)\n",
        "print(\"***********************Rule mining using exact mining***********************\")\n",
        "assoc_rules_exact_str = exactRule(assoc_rules_exact_str)\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_exact_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVQoaXQvldiq"
      },
      "source": [
        "# Exact rule mining and apriori algorithm\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "from python_utils import *\n",
        "from itertools import combinations, chain\n",
        "\n",
        "def count_occurences(itemset, Transactions):\n",
        "  count = 0\n",
        "  for i in range(len(Transactions)):\n",
        "    if set(itemset).issubset(set(Transactions[i])):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def get_frequent_apriori(itemsets, Transactions, min_support, prev_discarded):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  \n",
        "  k = len(prev_discarded.keys())\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    discarded_before = False\n",
        "    if k > 0:\n",
        "      for it in prev_discarded[k]:\n",
        "        if set(it).issubset(set(itemsets[s])):\n",
        "          discarded_before = True\n",
        "          break\n",
        "\n",
        "    if not discarded_before:\n",
        "      count = count_occurences(itemsets[s], Transactions)\n",
        "      if count/len(Transactions) >= min_support:\n",
        "        L.append(itemsets[s])\n",
        "        supp_count.append(count)\n",
        "      else:\n",
        "        new_discarded.append(itemsets[s])\n",
        "  return L , supp_count, new_discarded\n",
        "\n",
        "def get_frequent_exact(itemsets, Transactions):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  num_trans = len(Transactions)\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    count = count_occurences(itemsets[s], Transactions)\n",
        "    \n",
        "    L.append(itemsets[s])\n",
        "    supp_count.append(count)\n",
        "  return L , supp_count\n",
        "\n",
        "def load_transactions(path_to_data, order):\n",
        "    Transactions = []\n",
        "    with open(path_to_data, 'r') as fid:\n",
        "      for lines in fid:\n",
        "        str_line = list(lines.strip().split(','))\n",
        "        _t = list(np.unique(str_line))\n",
        "        _t.sort(key = lambda x: order.index(x))\n",
        "        Transactions.append(_t)\n",
        "    return Transactions\n",
        "\n",
        "def print_PIS_table(T, supp_count, PIS):\n",
        "  print(\"Itemset|Frequency|PIS\")\n",
        "  for k in range(len(T)):\n",
        "    # if(supp_count[k] <= PIS[k]):\n",
        "    print(\"{} : {} : {}\".format(T[k], supp_count[k], PIS[k]))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_MIS_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "arr2 = []\n",
        "\n",
        "def print_Exact_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Exact Itemsets\")\n",
        "  print(\"-------------------------------------------\")\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  count = 0\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    if(supp_count[k] == min(arr)):\n",
        "      arr2.append(T[k])\n",
        "      print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "      count += 1\n",
        "  if(count ==  0):\n",
        "        print(\"There is no exact itemsets\")\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_table(T, supp_count):\n",
        "    print(\"Itemset | Frequency\")\n",
        "    for k in range(len(T)):\n",
        "      print(\"{} : {}\".format(T[k], supp_count[k]))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "def join_two_itemsets(it1, it2, order):\n",
        "    it1.sort(key = lambda x: order.index(x))\n",
        "    it2.sort(key = lambda x: order.index(x))\n",
        "\n",
        "    for i in range(len(it1) - 1):\n",
        "      if it1[i] != it2[i]:\n",
        "        return []\n",
        "    if order.index(it1[-1]) < order.index(it2[-1]):\n",
        "      return it1 + [it2[-1]]\n",
        "\n",
        "    return []\n",
        "\n",
        "def join_set_itemsets(set_of_its, order):\n",
        "    C = []\n",
        "    for i in range(len(set_of_its)):\n",
        "      for j in range(i+1, len(set_of_its)):\n",
        "        it_out = join_two_itemsets(set_of_its[i], set_of_its[j], order)\n",
        "        if len(it_out) > 0:\n",
        "          C.append(it_out)\n",
        "    return C\n",
        "\n",
        "def powerset(s):\n",
        "  return list(chain.from_iterable(combinations(s,r) for r in range(1,len(s) + i)))\n",
        "\n",
        "\n",
        "def write_rules(X, X_S, S, conf, supp, num_trans):\n",
        "  out_rules = \"\"\n",
        "  out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "  out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "  out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "#   out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  \n",
        "  return out_rules\n",
        "\n",
        "Transactions = []\n",
        "# /content/drive/MyDrive/Pamir/tdb - Copy.csv\n",
        "with open('/content/drive/MyDrive/Pamir/tdb.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "for i in range(len(data)):\n",
        "  Transactions.append(data[i])\n",
        "\n",
        "num_trans = len(Transactions)\n",
        "# Transactions\n",
        "# Transactions\n",
        "orders = []\n",
        "orders.append(Transactions[0][0])\n",
        "for i in range(0,len(Transactions)):\n",
        "  for j in range(len(Transactions[i])):\n",
        "    count = 0\n",
        "    k = 0\n",
        "    while(k < len(orders)):\n",
        "      if Transactions[i][j] == orders[k]:\n",
        "        count += 1\n",
        "        break\n",
        "      k += 1\n",
        "    if count == 0:\n",
        "      orders.append(Transactions[i][j])\n",
        "\n",
        "order = sorted(orders)\n",
        "\n",
        "C = {}\n",
        "L = {}\n",
        "itemset_size = 1\n",
        "num_trans = len(Transactions)\n",
        "supp_count_L = {}\n",
        "C.update({itemset_size : [ [f] for f in order]})\n",
        "\n",
        "assoc_rules_apriori_str = \"\"\n",
        "assoc_rules_exact_str = \"\"\n",
        "def aprioriAlgo(assoc_rules_apriori_str):\n",
        "    min_support = 2/5\n",
        "    min_confidence = 1\n",
        "    Discarded = { itemset_size : []}\n",
        "    f, sup, new_discarded = get_frequent_apriori(C[itemset_size], Transactions, min_support, Discarded)\n",
        "    Discarded.update({itemset_size : new_discarded})\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})\n",
        "    print_table(L[1], supp_count_L[1])\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_table(C[k], [count_occurences(it, Transactions) for it in C[k]])\n",
        "      f,sup, new_discarded = get_frequent_apriori(C[k], Transactions, min_support,Discarded)\n",
        "      Discarded.update({k : new_discarded})\n",
        "      L.update({k : f})\n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 0:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print(\"Table L{}:\\n\".format(k))\n",
        "          print_table(L[k], supp_count_L[k])\n",
        "      k += 1\n",
        "\n",
        "    for i in range(1,len(L)):\n",
        "      for j in range(len(L[i])):\n",
        "        s = powerset(L[i][j])\n",
        "        s.pop()\n",
        "        for z in s:\n",
        "          S = set(z)\n",
        "          X = set(L[i][j])\n",
        "          X_S = set(X-S)\n",
        "          sup_x = count_occurences(X , Transactions)\n",
        "          sup_x_s = count_occurences(X_S, Transactions)\n",
        "          conf = sup_x / count_occurences(S, Transactions)\n",
        "          if conf >= min_confidence and sup_x >= min_support:\n",
        "              assoc_rules_apriori_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "\n",
        "    return assoc_rules_apriori_str\n",
        "\n",
        "def exactRule(assoc_rules_exact_str):\n",
        "    assoc_rules_str = \"\"\n",
        "    min_conf = 1\n",
        "    PIS1 = []\n",
        "    PIS = []\n",
        "    Dict = {}\n",
        "    f1, sup1 = get_frequent_exact(C[itemset_size], Transactions)\n",
        "    for i in range(len(C[itemset_size])):\n",
        "      n = int(input(\"Enter item PIS: \"))\n",
        "      PIS1.append(n)\n",
        "    f = []\n",
        "    sup = []\n",
        "    for i in range(0,len(PIS1)):\n",
        "      if(sup1[i] < PIS1[i]):\n",
        "        f.append(f1[i])\n",
        "        sup.append(sup1[i])\n",
        "        PIS.append(PIS1[i])\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})  \n",
        "    # L.update({itemset_size : f})\n",
        "    # supp_count_L.update({itemset_size : sup})\n",
        "    # print(L)\n",
        "    # print(supp_count_L)\n",
        "    # List is converting into string  \n",
        "    def convertList(list1):  \n",
        "    \n",
        "        return ' '.join([str(e) for e in list1]) #List comprehension  \n",
        "\n",
        "    for i in range(len(L[1])):\n",
        "        # case = {L[1][i] : supp_count_L[1][i]}\n",
        "        case = {convertList(L[1][i]) : supp_count_L[1][i]}\n",
        "        Dict.update(case)\n",
        "    # print(Dict)\n",
        "    \n",
        "    print_PIS_table(L[1], supp_count_L[1], PIS)\n",
        "\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      arr = []\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_MIS_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      print_Exact_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      f,sup = get_frequent_exact(C[k], Transactions)\n",
        "      L.update({k : f})\n",
        "      \n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 1:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print()\n",
        "      k += 1\n",
        "\n",
        "    # for i in range(1, len(L)):\n",
        "    for i in range(len(arr2)):\n",
        "      # for j in range(len(arr2[i])):\n",
        "      s = list(powerset(arr2[i]))\n",
        "      s.pop()\n",
        "      for z in s:\n",
        "        S = set(z)\n",
        "        X = set(arr2[i])\n",
        "        X_S = set(X-S)\n",
        "        sup_x = count_occurences(X , Transactions)\n",
        "        sup_x_s = count_occurences(X_S, Transactions)\n",
        "        conf = sup_x / count_occurences(S, Transactions)\n",
        "        if conf >= min_conf:\n",
        "          assoc_rules_exact_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "          \n",
        "    return assoc_rules_exact_str\n",
        "print(\"***********************Rule mining using apriori algorithm***********************\")\n",
        "assoc_rules_apriori_str = aprioriAlgo(assoc_rules_apriori_str)\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_apriori_str)\n",
        "print(\"***********************Rule mining using exact mining***********************\")\n",
        "assoc_rules_exact_str = exactRule(assoc_rules_exact_str)\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_exact_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1tOs9_XsRmM",
        "outputId": "70daf8e2-6f4e-4a6f-a51c-dfc6531d5d0f"
      },
      "source": [
        "f = open(\"/content/drive/MyDrive/Pamir/PIS_tdb.txt\", \"r\")\n",
        "arr = []\n",
        "for x in f:\n",
        "  arr.append(x)\n",
        "\n",
        "print(arr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0\\n', '0\\n', '1\\n', '2\\n', '5\\n', '6\\n', '7\\n', '8\\n', '9\\n', '0\\n', '0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30HH7ml7DFqr",
        "outputId": "442204a1-9ab9-462e-a329-e0a79df97d55"
      },
      "source": [
        "with open('/content/drive/MyDrive/Pamir/PIS_tdb.txt', 'r') as file:\n",
        "    all_file = file.read().strip()  # Read and remove any extra new line\n",
        "    all_file_list = all_file.split('\\n')  # make a list of lines\n",
        "    final_data = [int(each_int) for each_int in all_file_list]\n",
        "    print(final_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 1, 2, 5, 6, 7, 8, 9, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLtI7OT4DjWm",
        "outputId": "85e46ec8-0949-41fc-d75f-7393bea59a78"
      },
      "source": [
        "# Final project\n",
        "import csv\n",
        "import numpy as np\n",
        "from python_utils import *\n",
        "from itertools import combinations, chain\n",
        "import random\n",
        "import time\n",
        "\n",
        "def count_occurences(itemset, Transactions):\n",
        "  count = 0\n",
        "  for i in range(len(Transactions)):\n",
        "    if set(itemset).issubset(set(Transactions[i])):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def get_frequent_apriori(itemsets, Transactions, min_support, prev_discarded):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  \n",
        "  k = len(prev_discarded.keys())\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    discarded_before = False\n",
        "    if k > 0:\n",
        "      for it in prev_discarded[k]:\n",
        "        if set(it).issubset(set(itemsets[s])):\n",
        "          discarded_before = True\n",
        "          break\n",
        "\n",
        "    if not discarded_before:\n",
        "      count = count_occurences(itemsets[s], Transactions)\n",
        "      if count/len(Transactions) >= min_support:\n",
        "        L.append(itemsets[s])\n",
        "        supp_count.append(count)\n",
        "      else:\n",
        "        new_discarded.append(itemsets[s])\n",
        "  return L , supp_count, new_discarded\n",
        "\n",
        "def get_frequent_exact(itemsets, Transactions):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  num_trans = len(Transactions)\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    count = count_occurences(itemsets[s], Transactions)\n",
        "    \n",
        "    L.append(itemsets[s])\n",
        "    supp_count.append(count)\n",
        "  return L , supp_count\n",
        "\n",
        "def load_transactions(path_to_data, order):\n",
        "    Transactions = []\n",
        "    with open(path_to_data, 'r') as fid:\n",
        "      for lines in fid:\n",
        "        str_line = list(lines.strip().split(','))\n",
        "        _t = list(np.unique(str_line))\n",
        "        _t.sort(key = lambda x: order.index(x))\n",
        "        Transactions.append(_t)\n",
        "    return Transactions\n",
        "\n",
        "def print_PIS_table(T, supp_count, PIS):\n",
        "  print(\"Itemset|Frequency|PIS\")\n",
        "  for k in range(len(T)):\n",
        "    print(\"{} : {} : {}\".format(T[k], supp_count[k], PIS[k]))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_MIS_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "arr2 = []\n",
        "\n",
        "def print_Exact_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Exact Itemsets\")\n",
        "  print(\"-------------------------------------------\")\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  count = 0\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    if(supp_count[k] == min(arr)):\n",
        "      arr2.append(T[k])\n",
        "      print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "      count += 1\n",
        "  if(count ==  0):\n",
        "        print(\"There is no exact itemsets\")\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_table(T, supp_count):\n",
        "    print(\"Itemset | Frequency\")\n",
        "    for k in range(len(T)):\n",
        "      print(\"{} : {}\".format(T[k], supp_count[k]))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "def join_two_itemsets(it1, it2, order):\n",
        "    it1.sort(key = lambda x: order.index(x))\n",
        "    it2.sort(key = lambda x: order.index(x))\n",
        "\n",
        "    for i in range(len(it1) - 1):\n",
        "      if it1[i] != it2[i]:\n",
        "        return []\n",
        "    if order.index(it1[-1]) < order.index(it2[-1]):\n",
        "      return it1 + [it2[-1]]\n",
        "\n",
        "    return []\n",
        "\n",
        "def join_set_itemsets(set_of_its, order):\n",
        "    C = []\n",
        "    for i in range(len(set_of_its)):\n",
        "      for j in range(i+1, len(set_of_its)):\n",
        "        it_out = join_two_itemsets(set_of_its[i], set_of_its[j], order)\n",
        "        if len(it_out) > 0:\n",
        "          C.append(it_out)\n",
        "    return C\n",
        "\n",
        "def powerset(s):\n",
        "  return list(chain.from_iterable(combinations(s,r) for r in range(1,len(s) + i)))\n",
        "\n",
        "\n",
        "def write_rules(X, X_S, S, conf, supp, num_trans):\n",
        "  out_rules = \"\"\n",
        "  out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "  out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "  out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "  # out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  \n",
        "  return out_rules\n",
        "\n",
        "def write_rules_lift(X, X_S, S, conf, fre,num_trans):\n",
        "  out_rules = \"\"\n",
        "  lift = conf / fre\n",
        "  if(lift >= 0.25):\n",
        "    out_rules = \"\"\n",
        "    out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "    out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "    out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "    out_rules += \"     Lift: {0:2.3f} \\n\\n\".format(lift)\n",
        "    # out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  return out_rules\n",
        "\n",
        "Transactions = []\n",
        "filePath = input(\"Enter file path: \")\n",
        "with open(filePath, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "for i in range(len(data)):\n",
        "  Transactions.append(data[i])\n",
        "\n",
        "num_trans = len(Transactions)\n",
        "# Transactions\n",
        "# Transactions\n",
        "orders = []\n",
        "orders.append(Transactions[0][0])\n",
        "for i in range(0,len(Transactions)):\n",
        "  for j in range(len(Transactions[i])):\n",
        "    count = 0\n",
        "    k = 0\n",
        "    while(k < len(orders)):\n",
        "      if Transactions[i][j] == orders[k]:\n",
        "        count += 1\n",
        "        break\n",
        "      k += 1\n",
        "    if count == 0:\n",
        "      orders.append(Transactions[i][j])\n",
        "\n",
        "order = sorted(orders)\n",
        "print(len(orders))\n",
        "C = {}\n",
        "L = {}\n",
        "itemset_size = 1\n",
        "num_trans = len(Transactions)\n",
        "supp_count_L = {}\n",
        "C.update({itemset_size : [ [f] for f in order]})\n",
        "\n",
        "assoc_rules_apriori_str = \"\"\n",
        "assoc_rules_apriori_str_lift = \"\"\n",
        "assoc_rules_exact_str = \"\"\n",
        "assoc_rules_apriori_str_exact_lift = \"\"\n",
        "def aprioriAlgo(assoc_rules_apriori_str,assoc_rules_apriori_str_lift):\n",
        "    timeTaking = time.time()\n",
        "    min_support = float(input(\"Enter minimum support: \"))\n",
        "    min_confidence = 1\n",
        "    Discarded = { itemset_size : []}\n",
        "    f, sup, new_discarded = get_frequent_apriori(C[itemset_size], Transactions, min_support, Discarded)\n",
        "    Discarded.update({itemset_size : new_discarded})\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})\n",
        "    print_table(L[1], supp_count_L[1])\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_table(C[k], [count_occurences(it, Transactions) for it in C[k]])\n",
        "      f,sup, new_discarded = get_frequent_apriori(C[k], Transactions, min_support,Discarded)\n",
        "      Discarded.update({k : new_discarded})\n",
        "      L.update({k : f})\n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 0:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print(\"Table L{}:\\n\".format(k))\n",
        "          print_table(L[k], supp_count_L[k])\n",
        "      k += 1\n",
        "    for i in range(1,len(L)):\n",
        "      for j in range(len(L[i])):\n",
        "        s = powerset(L[i][j])\n",
        "        s.pop()\n",
        "        for z in s:\n",
        "          S = set(z)\n",
        "          X = set(L[i][j])\n",
        "          X_S = set(X-S)\n",
        "          sup_x = count_occurences(X , Transactions)\n",
        "          sup_x_s = count_occurences(X_S, Transactions)\n",
        "          conf = sup_x / count_occurences(S, Transactions)\n",
        "          if conf >= min_confidence and sup_x >= min_support:\n",
        "              assoc_rules_apriori_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "              assoc_rules_apriori_str_lift += write_rules_lift(X, X_S, S, conf, sup_x_s,num_trans)\n",
        "    \n",
        "    timeTaking1 = time.time()\n",
        "    totalTime = timeTaking1 - timeTaking\n",
        "    print(\"Time taking for apriori: \" + str(totalTime))\n",
        "    return assoc_rules_apriori_str,assoc_rules_apriori_str_lift\n",
        "\n",
        "def exactRule(assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift):\n",
        "    timeTaking = time.time()\n",
        "    assoc_rules_str = \"\"\n",
        "    min_conf = 1\n",
        "    PIS1 = []\n",
        "    PIS = []\n",
        "    Dict = {}\n",
        "    f1, sup1 = get_frequent_exact(C[itemset_size], Transactions)\n",
        "\n",
        "    PISFilePath = input(\"Enter PIS file path: \")\n",
        "    f = open(PISFilePath, \"w\")\n",
        "    for i in range(0,len(order)):\n",
        "      f.write(str(random.randrange(0, 5)) + \"\\n\")\n",
        "    f.close()\n",
        "\n",
        "    with open(PISFilePath, 'r') as file:\n",
        "      all_file = file.read().strip()  # Read and remove any extra new line\n",
        "      all_file_list = all_file.split('\\n')  # make a list of lines\n",
        "      PIS1 = [int(each_int) for each_int in all_file_list]\n",
        "    \n",
        "    f = []\n",
        "    sup = []\n",
        "    for i in range(0,len(PIS1)):\n",
        "      if(sup1[i] < PIS1[i]):\n",
        "        f.append(f1[i])\n",
        "        sup.append(sup1[i])\n",
        "        PIS.append(PIS1[i])\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})  \n",
        "\n",
        "    # List is converting into string  \n",
        "    def convertList(list1):  \n",
        "    \n",
        "        return ' '.join([str(e) for e in list1]) #List comprehension  \n",
        "\n",
        "    for i in range(len(L[1])):\n",
        "        # case = {L[1][i] : supp_count_L[1][i]}\n",
        "        case = {convertList(L[1][i]) : supp_count_L[1][i]}\n",
        "        Dict.update(case)\n",
        "    # print(Dict)\n",
        "    print_PIS_table(L[1], supp_count_L[1], PIS)\n",
        "\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      arr = []\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_MIS_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      print_Exact_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      f,sup = get_frequent_exact(C[k], Transactions)\n",
        "      L.update({k : f})\n",
        "      \n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 1:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print()\n",
        "      k += 1\n",
        "\n",
        "    # for i in range(1, len(L)):\n",
        "    for i in range(len(arr2)):\n",
        "      # for j in range(len(arr2[i])):\n",
        "      s = list(powerset(arr2[i]))\n",
        "      s.pop()\n",
        "      for z in s:\n",
        "        S = set(z)\n",
        "        X = set(arr2[i])\n",
        "        X_S = set(X-S)\n",
        "        sup_x = count_occurences(X , Transactions)\n",
        "        sup_x_s = count_occurences(X_S, Transactions)\n",
        "        conf = sup_x / count_occurences(S, Transactions)\n",
        "        if conf >= min_conf:\n",
        "          assoc_rules_exact_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "          assoc_rules_apriori_str_exact_lift += write_rules_lift(X, X_S, S, conf, sup_x_s,num_trans)\n",
        "    \n",
        "    timeTaking1 = time.time()\n",
        "    totalTime = timeTaking1 - timeTaking\n",
        "    print(\"Time taking for Exact mining: \" + str(totalTime))\n",
        "    return assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift\n",
        "\n",
        "print(\"********Rule mining using apriori algorithm********\")\n",
        "assoc_rules_apriori_str,assoc_rules_apriori_str_lift = aprioriAlgo(assoc_rules_apriori_str,assoc_rules_apriori_str_lift)\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_apriori_str)\n",
        "# print(\"-----------------------Association Rule With Lift-------------------\\n\")\n",
        "# print(assoc_rules_apriori_str_lift)\n",
        "# print(\"********Rule mining using exact mining********\")\n",
        "# assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift = exactRule(assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift)\n",
        "# print(\"-----------------------Association Rule-------------------\\n\")\n",
        "# print(assoc_rules_exact_str)\n",
        "# print(\"-----------------------Association Rule With Lift-------------------\\n\")\n",
        "# print(assoc_rules_apriori_str_exact_lift)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter file path: /content/drive/MyDrive/Pamir/tdb.csv\n",
            "11\n",
            "********Rule mining using apriori algorithm********\n",
            "Enter minimum support: 0.6\n",
            "Itemset | Frequency\n",
            "['E'] : 4\n",
            "['K'] : 5\n",
            "['M'] : 3\n",
            "['O'] : 3\n",
            "['Y'] : 3\n",
            "\n",
            "\n",
            "\n",
            "Table C2:\n",
            "\n",
            "Itemset | Frequency\n",
            "['E', 'K'] : 4\n",
            "['E', 'M'] : 2\n",
            "['E', 'O'] : 3\n",
            "['E', 'Y'] : 2\n",
            "['K', 'M'] : 3\n",
            "['K', 'O'] : 3\n",
            "['K', 'Y'] : 3\n",
            "['M', 'O'] : 1\n",
            "['M', 'Y'] : 2\n",
            "['O', 'Y'] : 2\n",
            "\n",
            "\n",
            "\n",
            "Table L2:\n",
            "\n",
            "Itemset | Frequency\n",
            "['E', 'K'] : 4\n",
            "['E', 'O'] : 3\n",
            "['K', 'M'] : 3\n",
            "['K', 'O'] : 3\n",
            "['K', 'Y'] : 3\n",
            "\n",
            "\n",
            "\n",
            "Table C3:\n",
            "\n",
            "Itemset | Frequency\n",
            "['E', 'K', 'O'] : 3\n",
            "['K', 'M', 'O'] : 1\n",
            "['K', 'M', 'Y'] : 2\n",
            "['K', 'O', 'Y'] : 2\n",
            "\n",
            "\n",
            "\n",
            "Table L3:\n",
            "\n",
            "Itemset | Frequency\n",
            "['E', 'K', 'O'] : 3\n",
            "\n",
            "\n",
            "\n",
            "Table C4:\n",
            "\n",
            "Itemset | Frequency\n",
            "\n",
            "\n",
            "\n",
            "Time taking for apriori: 3.04026460647583\n",
            "-----------------------Association Rule-------------------\n",
            "\n",
            "Freq. Itemset: {'E', 'K'}\n",
            "     Rules: ['E'] -> ['K'] \n",
            "     Conf: 1.000 \n",
            "\n",
            "Freq. Itemset: {'E', 'O'}\n",
            "     Rules: ['O'] -> ['E'] \n",
            "     Conf: 1.000 \n",
            "\n",
            "Freq. Itemset: {'M', 'K'}\n",
            "     Rules: ['M'] -> ['K'] \n",
            "     Conf: 1.000 \n",
            "\n",
            "Freq. Itemset: {'K', 'O'}\n",
            "     Rules: ['O'] -> ['K'] \n",
            "     Conf: 1.000 \n",
            "\n",
            "Freq. Itemset: {'K', 'Y'}\n",
            "     Rules: ['Y'] -> ['K'] \n",
            "     Conf: 1.000 \n",
            "\n",
            "Freq. Itemset: {'E', 'O', 'K'}\n",
            "     Rules: ['O'] -> ['E', 'K'] \n",
            "     Conf: 1.000 \n",
            "\n",
            "Freq. Itemset: {'E', 'O', 'K'}\n",
            "     Rules: ['E', 'O'] -> ['K'] \n",
            "     Conf: 1.000 \n",
            "\n",
            "Freq. Itemset: {'E', 'O', 'K'}\n",
            "     Rules: ['K', 'O'] -> ['E'] \n",
            "     Conf: 1.000 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEWuAqh7wHSe",
        "outputId": "99a9c854-0b3a-47de-d2bd-eac3994088e6"
      },
      "source": [
        "% cd '/content'\n",
        "!git clone https://github.com/pamirghosh/DataMining"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'DataMining'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHfDPCEYts7F"
      },
      "source": [
        "# Apriori and Exact rule where support >= min_support\n",
        "import csv\n",
        "import numpy as np\n",
        "from python_utils import *\n",
        "from itertools import combinations, chain\n",
        "import random\n",
        "\n",
        "def count_occurences(itemset, Transactions):\n",
        "  count = 0\n",
        "  for i in range(len(Transactions)):\n",
        "    if set(itemset).issubset(set(Transactions[i])):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def get_frequent_apriori(itemsets, Transactions, min_support, prev_discarded):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  \n",
        "  k = len(prev_discarded.keys())\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    discarded_before = False\n",
        "    if k > 0:\n",
        "      for it in prev_discarded[k]:\n",
        "        if set(it).issubset(set(itemsets[s])):\n",
        "          discarded_before = True\n",
        "          break\n",
        "\n",
        "    if not discarded_before:\n",
        "      count = count_occurences(itemsets[s], Transactions)\n",
        "      if count/len(Transactions) >= min_support:\n",
        "        L.append(itemsets[s])\n",
        "        supp_count.append(count)\n",
        "      else:\n",
        "        new_discarded.append(itemsets[s])\n",
        "  return L , supp_count, new_discarded\n",
        "\n",
        "def get_frequent_exact(itemsets, Transactions):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  num_trans = len(Transactions)\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    count = count_occurences(itemsets[s], Transactions)\n",
        "    \n",
        "    L.append(itemsets[s])\n",
        "    supp_count.append(count)\n",
        "  return L , supp_count\n",
        "\n",
        "def load_transactions(path_to_data, order):\n",
        "    Transactions = []\n",
        "    with open(path_to_data, 'r') as fid:\n",
        "      for lines in fid:\n",
        "        str_line = list(lines.strip().split(','))\n",
        "        _t = list(np.unique(str_line))\n",
        "        _t.sort(key = lambda x: order.index(x))\n",
        "        Transactions.append(_t)\n",
        "    return Transactions\n",
        "\n",
        "def print_PIS_table(T, supp_count, PIS):\n",
        "  print(\"Itemset|Frequency|PIS\")\n",
        "  for k in range(len(T)):\n",
        "    print(\"{} : {} : {}\".format(T[k], supp_count[k], PIS[k]))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_MIS_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "arr2 = []\n",
        "\n",
        "def print_Exact_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Exact Itemsets\")\n",
        "  print(\"-------------------------------------------\")\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  count = 0\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    if(supp_count[k] == min(arr)):\n",
        "      arr2.append(T[k])\n",
        "      print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "      count += 1\n",
        "  if(count ==  0):\n",
        "        print(\"There is no exact itemsets\")\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_table(T, supp_count):\n",
        "    print(\"Itemset | Frequency\")\n",
        "    for k in range(len(T)):\n",
        "      print(\"{} : {}\".format(T[k], supp_count[k]))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "def join_two_itemsets(it1, it2, order):\n",
        "    it1.sort(key = lambda x: order.index(x))\n",
        "    it2.sort(key = lambda x: order.index(x))\n",
        "\n",
        "    for i in range(len(it1) - 1):\n",
        "      if it1[i] != it2[i]:\n",
        "        return []\n",
        "    if order.index(it1[-1]) < order.index(it2[-1]):\n",
        "      return it1 + [it2[-1]]\n",
        "\n",
        "    return []\n",
        "\n",
        "def join_set_itemsets(set_of_its, order):\n",
        "    C = []\n",
        "    for i in range(len(set_of_its)):\n",
        "      for j in range(i+1, len(set_of_its)):\n",
        "        it_out = join_two_itemsets(set_of_its[i], set_of_its[j], order)\n",
        "        if len(it_out) > 0:\n",
        "          C.append(it_out)\n",
        "    return C\n",
        "\n",
        "def powerset(s):\n",
        "  return list(chain.from_iterable(combinations(s,r) for r in range(1,len(s) + i)))\n",
        "\n",
        "\n",
        "def write_rules(X, X_S, S, conf, supp, num_trans):\n",
        "  out_rules = \"\"\n",
        "  out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "  out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "  out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "  out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  \n",
        "  return out_rules\n",
        "\n",
        "def write_rules_lift(X, X_S, S, conf, fre,num_trans):\n",
        "  out_rules = \"\"\n",
        "  lift = conf / fre\n",
        "  if(lift >= 0.25):\n",
        "    out_rules = \"\"\n",
        "    out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "    out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "    out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "    out_rules += \"     Lift: {0:2.3f} \\n\\n\".format(lift)\n",
        "    # out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  return out_rules\n",
        "\n",
        "Transactions = []\n",
        "filePath = input(\"Enter file path: \")\n",
        "with open(filePath, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "for i in range(len(data)):\n",
        "  Transactions.append(data[i])\n",
        "\n",
        "num_trans = len(Transactions)\n",
        "# Transactions\n",
        "# Transactions\n",
        "orders = []\n",
        "orders.append(Transactions[0][0])\n",
        "for i in range(0,len(Transactions)):\n",
        "  for j in range(len(Transactions[i])):\n",
        "    count = 0\n",
        "    k = 0\n",
        "    while(k < len(orders)):\n",
        "      if Transactions[i][j] == orders[k]:\n",
        "        count += 1\n",
        "        break\n",
        "      k += 1\n",
        "    if count == 0:\n",
        "      orders.append(Transactions[i][j])\n",
        "\n",
        "order = sorted(orders)\n",
        "print(len(orders))\n",
        "C = {}\n",
        "L = {}\n",
        "itemset_size = 1\n",
        "num_trans = len(Transactions)\n",
        "supp_count_L = {}\n",
        "C.update({itemset_size : [ [f] for f in order]})\n",
        "\n",
        "assoc_rules_apriori_str = \"\"\n",
        "assoc_rules_apriori_str_lift = \"\"\n",
        "assoc_rules_exact_str = \"\"\n",
        "assoc_rules_apriori_str_exact_lift = \"\"\n",
        "def aprioriAlgo(assoc_rules_apriori_str,assoc_rules_apriori_str_lift):\n",
        "    min_support = float(input(\"Enter minimum support: \"))\n",
        "    min_confidence = 1\n",
        "    Discarded = { itemset_size : []}\n",
        "    f, sup, new_discarded = get_frequent_apriori(C[itemset_size], Transactions, min_support, Discarded)\n",
        "    Discarded.update({itemset_size : new_discarded})\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})\n",
        "    print_table(L[1], supp_count_L[1])\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_table(C[k], [count_occurences(it, Transactions) for it in C[k]])\n",
        "      f,sup, new_discarded = get_frequent_apriori(C[k], Transactions, min_support,Discarded)\n",
        "      Discarded.update({k : new_discarded})\n",
        "      L.update({k : f})\n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 0:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print(\"Table L{}:\\n\".format(k))\n",
        "          print_table(L[k], supp_count_L[k])\n",
        "      k += 1\n",
        "    for i in range(1,len(L)):\n",
        "      for j in range(len(L[i])):\n",
        "        s = powerset(L[i][j])\n",
        "        s.pop()\n",
        "        for z in s:\n",
        "          S = set(z)\n",
        "          X = set(L[i][j])\n",
        "          X_S = set(X-S)\n",
        "          sup_x = count_occurences(X , Transactions)\n",
        "          sup_x_s = count_occurences(X_S, Transactions)\n",
        "          conf = sup_x / count_occurences(S, Transactions)\n",
        "          if conf >= min_confidence and sup_x >= min_support:\n",
        "              assoc_rules_apriori_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "              assoc_rules_apriori_str_lift += write_rules_lift(X, X_S, S, conf, sup_x_s,num_trans)\n",
        "    return assoc_rules_apriori_str,assoc_rules_apriori_str_lift\n",
        "\n",
        "def exactRule(assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift):\n",
        "    assoc_rules_str = \"\"\n",
        "    min_conf = 1\n",
        "    PIS1 = []\n",
        "    PIS = []\n",
        "    Dict = {}\n",
        "    f1, sup1 = get_frequent_exact(C[itemset_size], Transactions)\n",
        "\n",
        "    PISFilePath = input(\"Enter PIS file path: \")\n",
        "    f = open(PISFilePath, \"w\")\n",
        "    for i in range(0,len(order)):\n",
        "      f.write(str(random.randrange(0, 20)) + \"\\n\")\n",
        "    f.close()\n",
        "\n",
        "    with open(PISFilePath, 'r') as file:\n",
        "      all_file = file.read().strip()  # Read and remove any extra new line\n",
        "      all_file_list = all_file.split('\\n')  # make a list of lines\n",
        "      PIS1 = [int(each_int) for each_int in all_file_list]\n",
        "    \n",
        "    f = []\n",
        "    sup = []\n",
        "    for i in range(0,len(PIS1)):\n",
        "      if(sup1[i] < PIS1[i]):\n",
        "        f.append(f1[i])\n",
        "        sup.append(sup1[i])\n",
        "        PIS.append(PIS1[i])\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})  \n",
        "\n",
        "    # List is converting into string  \n",
        "    def convertList(list1):  \n",
        "    \n",
        "        return ' '.join([str(e) for e in list1]) #List comprehension  \n",
        "\n",
        "    for i in range(len(L[1])):\n",
        "        # case = {L[1][i] : supp_count_L[1][i]}\n",
        "        case = {convertList(L[1][i]) : supp_count_L[1][i]}\n",
        "        Dict.update(case)\n",
        "    # print(Dict)\n",
        "    print_PIS_table(L[1], supp_count_L[1], PIS)\n",
        "\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      arr = []\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_MIS_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      print_Exact_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      f,sup = get_frequent_exact(C[k], Transactions)\n",
        "      L.update({k : f})\n",
        "      \n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 1:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print()\n",
        "      k += 1\n",
        "\n",
        "    # for i in range(1, len(L)):\n",
        "    for i in range(len(arr2)):\n",
        "      # for j in range(len(arr2[i])):\n",
        "      s = list(powerset(arr2[i]))\n",
        "      s.pop()\n",
        "      for z in s:\n",
        "        S = set(z)\n",
        "        X = set(arr2[i])\n",
        "        X_S = set(X-S)\n",
        "        sup_x = count_occurences(X , Transactions)\n",
        "        sup_x_s = count_occurences(X_S, Transactions)\n",
        "        conf = sup_x / count_occurences(S, Transactions)\n",
        "        if conf >= min_conf:\n",
        "          assoc_rules_exact_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "          assoc_rules_apriori_str_exact_lift += write_rules_lift(X, X_S, S, conf, sup_x_s,num_trans)\n",
        "    return assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift\n",
        "\n",
        "print(\"***Rule mining using apriori algorithm***\")\n",
        "assoc_rules_apriori_str,assoc_rules_apriori_str_lift = aprioriAlgo(assoc_rules_apriori_str,assoc_rules_apriori_str_lift)\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_apriori_str)\n",
        "print(\"-----------------------Association Rule With Lift-------------------\\n\")\n",
        "print(assoc_rules_apriori_str_lift)\n",
        "# print(\"***Rule mining using exact mining***\")\n",
        "# assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift = exactRule(assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift)\n",
        "# print(\"-----------------------Association Rule-------------------\\n\")\n",
        "# print(assoc_rules_exact_str)\n",
        "# print(\"-----------------------Association Rule With Lift-------------------\\n\")\n",
        "# print(assoc_rules_apriori_str_exact_lift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnnJY5omIksB"
      },
      "source": [
        "# Apriori and Exact where support <= min_support\n",
        "import csv\n",
        "import numpy as np\n",
        "from python_utils import *\n",
        "from itertools import combinations, chain\n",
        "import random\n",
        "\n",
        "def count_occurences(itemset, Transactions):\n",
        "  count = 0\n",
        "  for i in range(len(Transactions)):\n",
        "    if set(itemset).issubset(set(Transactions[i])):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def get_frequent_apriori(itemsets, Transactions, min_support, prev_discarded):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  \n",
        "  k = len(prev_discarded.keys())\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    discarded_before = False\n",
        "    if k > 0:\n",
        "      for it in prev_discarded[k]:\n",
        "        if set(it).issubset(set(itemsets[s])):\n",
        "          discarded_before = True\n",
        "          break\n",
        "\n",
        "    if not discarded_before:\n",
        "      count = count_occurences(itemsets[s], Transactions)\n",
        "      if count/len(Transactions) <= min_support:\n",
        "        L.append(itemsets[s])\n",
        "        supp_count.append(count)\n",
        "      else:\n",
        "        new_discarded.append(itemsets[s])\n",
        "  return L , supp_count, new_discarded\n",
        "\n",
        "def get_frequent_exact(itemsets, Transactions):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  num_trans = len(Transactions)\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    count = count_occurences(itemsets[s], Transactions)\n",
        "    \n",
        "    L.append(itemsets[s])\n",
        "    supp_count.append(count)\n",
        "  return L , supp_count\n",
        "\n",
        "def load_transactions(path_to_data, order):\n",
        "    Transactions = []\n",
        "    with open(path_to_data, 'r') as fid:\n",
        "      for lines in fid:\n",
        "        str_line = list(lines.strip().split(','))\n",
        "        _t = list(np.unique(str_line))\n",
        "        _t.sort(key = lambda x: order.index(x))\n",
        "        Transactions.append(_t)\n",
        "    return Transactions\n",
        "\n",
        "def print_PIS_table(T, supp_count, PIS):\n",
        "  print(\"Itemset|Frequency|PIS\")\n",
        "  for k in range(len(T)):\n",
        "    print(\"{} : {} : {}\".format(T[k], supp_count[k], PIS[k]))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_MIS_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "arr2 = []\n",
        "\n",
        "def print_Exact_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Exact Itemsets\")\n",
        "  print(\"-------------------------------------------\")\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  count = 0\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    if(supp_count[k] == min(arr)):\n",
        "      arr2.append(T[k])\n",
        "      print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "      count += 1\n",
        "  if(count ==  0):\n",
        "        print(\"There is no exact itemsets\")\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_table(T, supp_count):\n",
        "    print(\"Itemset | Frequency\")\n",
        "    for k in range(len(T)):\n",
        "      print(\"{} : {}\".format(T[k], supp_count[k]))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "def join_two_itemsets(it1, it2, order):\n",
        "    it1.sort(key = lambda x: order.index(x))\n",
        "    it2.sort(key = lambda x: order.index(x))\n",
        "\n",
        "    for i in range(len(it1) - 1):\n",
        "      if it1[i] != it2[i]:\n",
        "        return []\n",
        "    if order.index(it1[-1]) < order.index(it2[-1]):\n",
        "      return it1 + [it2[-1]]\n",
        "\n",
        "    return []\n",
        "\n",
        "def join_set_itemsets(set_of_its, order):\n",
        "    C = []\n",
        "    for i in range(len(set_of_its)):\n",
        "      for j in range(i+1, len(set_of_its)):\n",
        "        it_out = join_two_itemsets(set_of_its[i], set_of_its[j], order)\n",
        "        if len(it_out) > 0:\n",
        "          C.append(it_out)\n",
        "    return C\n",
        "\n",
        "def powerset(s):\n",
        "  return list(chain.from_iterable(combinations(s,r) for r in range(1,len(s) + i)))\n",
        "\n",
        "\n",
        "def write_rules(X, X_S, S, conf, supp, num_trans):\n",
        "  out_rules = \"\"\n",
        "  out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "  out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "  out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "  out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  \n",
        "  return out_rules\n",
        "\n",
        "def write_rules_lift(X, X_S, S, conf, fre,num_trans):\n",
        "  out_rules = \"\"\n",
        "  lift = conf / fre\n",
        "  if(lift >= 0.25):\n",
        "    out_rules = \"\"\n",
        "    out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "    out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "    out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "    out_rules += \"     Lift: {0:2.3f} \\n\\n\".format(lift)\n",
        "    # out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  return out_rules\n",
        "\n",
        "Transactions = []\n",
        "filePath = input(\"Enter file path: \")\n",
        "with open(filePath, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "for i in range(len(data)):\n",
        "  Transactions.append(data[i])\n",
        "\n",
        "num_trans = len(Transactions)\n",
        "# Transactions\n",
        "# Transactions\n",
        "orders = []\n",
        "orders.append(Transactions[0][0])\n",
        "for i in range(0,len(Transactions)):\n",
        "  for j in range(len(Transactions[i])):\n",
        "    count = 0\n",
        "    k = 0\n",
        "    while(k < len(orders)):\n",
        "      if Transactions[i][j] == orders[k]:\n",
        "        count += 1\n",
        "        break\n",
        "      k += 1\n",
        "    if count == 0:\n",
        "      orders.append(Transactions[i][j])\n",
        "\n",
        "order = sorted(orders)\n",
        "print(len(orders))\n",
        "C = {}\n",
        "L = {}\n",
        "itemset_size = 1\n",
        "num_trans = len(Transactions)\n",
        "supp_count_L = {}\n",
        "C.update({itemset_size : [ [f] for f in order]})\n",
        "\n",
        "assoc_rules_apriori_str = \"\"\n",
        "assoc_rules_apriori_str_lift = \"\"\n",
        "assoc_rules_exact_str = \"\"\n",
        "assoc_rules_apriori_str_exact_lift = \"\"\n",
        "def aprioriAlgo(assoc_rules_apriori_str,assoc_rules_apriori_str_lift):\n",
        "    min_support = float(input(\"Enter minimum support: \"))\n",
        "    min_confidence = 1\n",
        "    Discarded = { itemset_size : []}\n",
        "    f, sup, new_discarded = get_frequent_apriori(C[itemset_size], Transactions, min_support, Discarded)\n",
        "    Discarded.update({itemset_size : new_discarded})\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})\n",
        "    print_table(L[1], supp_count_L[1])\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_table(C[k], [count_occurences(it, Transactions) for it in C[k]])\n",
        "      f,sup, new_discarded = get_frequent_apriori(C[k], Transactions, min_support,Discarded)\n",
        "      Discarded.update({k : new_discarded})\n",
        "      L.update({k : f})\n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 0:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print(\"Table L{}:\\n\".format(k))\n",
        "          print_table(L[k], supp_count_L[k])\n",
        "      k += 1\n",
        "    for i in range(1,len(L)):\n",
        "      for j in range(len(L[i])):\n",
        "        s = powerset(L[i][j])\n",
        "        s.pop()\n",
        "        for z in s:\n",
        "          S = set(z)\n",
        "          X = set(L[i][j])\n",
        "          X_S = set(X-S)\n",
        "          sup_x = count_occurences(X , Transactions)\n",
        "          sup_x_s = count_occurences(X_S, Transactions)\n",
        "          co = count_occurences(S, Transactions)\n",
        "          if co > 0:\n",
        "            conf = sup_x / co\n",
        "            if conf >= min_confidence:\n",
        "              assoc_rules_apriori_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "              assoc_rules_apriori_str_lift += write_rules_lift(X, X_S, S, conf, sup_x_s,num_trans)\n",
        "    return assoc_rules_apriori_str,assoc_rules_apriori_str_lift\n",
        "\n",
        "def exactRule(assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift):\n",
        "    assoc_rules_str = \"\"\n",
        "    min_conf = 1\n",
        "    PIS1 = []\n",
        "    PIS = []\n",
        "    Dict = {}\n",
        "    f1, sup1 = get_frequent_exact(C[itemset_size], Transactions)\n",
        "\n",
        "    PISFilePath = input(\"Enter PIS file path: \")\n",
        "    f = open(PISFilePath, \"w\")\n",
        "    for i in range(0,len(order)):\n",
        "      f.write(str(random.randrange(0, 20)) + \"\\n\")\n",
        "    f.close()\n",
        "\n",
        "    with open(PISFilePath, 'r') as file:\n",
        "      all_file = file.read().strip()  # Read and remove any extra new line\n",
        "      all_file_list = all_file.split('\\n')  # make a list of lines\n",
        "      PIS1 = [int(each_int) for each_int in all_file_list]\n",
        "    \n",
        "    f = []\n",
        "    sup = []\n",
        "    for i in range(0,len(PIS1)):\n",
        "      if(sup1[i] < PIS1[i]):\n",
        "        f.append(f1[i])\n",
        "        sup.append(sup1[i])\n",
        "        PIS.append(PIS1[i])\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})  \n",
        "\n",
        "    # List is converting into string  \n",
        "    def convertList(list1):  \n",
        "    \n",
        "        return ' '.join([str(e) for e in list1]) #List comprehension  \n",
        "\n",
        "    for i in range(len(L[1])):\n",
        "        # case = {L[1][i] : supp_count_L[1][i]}\n",
        "        case = {convertList(L[1][i]) : supp_count_L[1][i]}\n",
        "        Dict.update(case)\n",
        "    # print(Dict)\n",
        "    print_PIS_table(L[1], supp_count_L[1], PIS)\n",
        "\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      arr = []\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_MIS_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      print_Exact_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      f,sup = get_frequent_exact(C[k], Transactions)\n",
        "      L.update({k : f})\n",
        "      \n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 1:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print()\n",
        "      k += 1\n",
        "\n",
        "    # for i in range(1, len(L)):\n",
        "    for i in range(len(arr2)):\n",
        "      # for j in range(len(arr2[i])):\n",
        "      s = list(powerset(arr2[i]))\n",
        "      s.pop()\n",
        "      for z in s:\n",
        "        S = set(z)\n",
        "        X = set(arr2[i])\n",
        "        X_S = set(X-S)\n",
        "        sup_x = count_occurences(X , Transactions)\n",
        "        sup_x_s = count_occurences(X_S, Transactions)\n",
        "        conf = sup_x / count_occurences(S, Transactions)\n",
        "        if conf >= min_conf:\n",
        "          assoc_rules_exact_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "          assoc_rules_apriori_str_exact_lift += write_rules_lift(X, X_S, S, conf, sup_x_s,num_trans)\n",
        "    return assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift\n",
        "\n",
        "# print(\"***Rule mining using apriori algorithm***\")\n",
        "# assoc_rules_apriori_str,assoc_rules_apriori_str_lift = aprioriAlgo(assoc_rules_apriori_str,assoc_rules_apriori_str_lift)\n",
        "# print(\"-----------------------Association Rule-------------------\\n\")\n",
        "# print(assoc_rules_apriori_str)\n",
        "# print(\"-----------------------Association Rule With Lift-------------------\\n\")\n",
        "# print(assoc_rules_apriori_str_lift)\n",
        "print(\"***Rule mining using exact mining***\")\n",
        "assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift = exactRule(assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift)\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_exact_str)\n",
        "print(\"-----------------------Association Rule With Lift-------------------\\n\")\n",
        "print(assoc_rules_apriori_str_exact_lift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNKx1M9sLfMO"
      },
      "source": [
        "# Apriori and Exact where support <= min_support\n",
        "import csv\n",
        "import numpy as np\n",
        "from python_utils import *\n",
        "from itertools import combinations, chain\n",
        "import random\n",
        "\n",
        "def count_occurences(itemset, Transactions):\n",
        "  count = 0\n",
        "  for i in range(len(Transactions)):\n",
        "    if set(itemset).issubset(set(Transactions[i])):\n",
        "      count += 1\n",
        "  return count\n",
        "\n",
        "def get_frequent_apriori(itemsets, Transactions, min_support, prev_discarded):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  \n",
        "  k = len(prev_discarded.keys())\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    discarded_before = False\n",
        "    if k > 0:\n",
        "      for it in prev_discarded[k]:\n",
        "        if set(it).issubset(set(itemsets[s])):\n",
        "          discarded_before = True\n",
        "          break\n",
        "\n",
        "    if not discarded_before:\n",
        "      count = count_occurences(itemsets[s], Transactions)\n",
        "      if count/len(Transactions) <= min_support:\n",
        "        L.append(itemsets[s])\n",
        "        supp_count.append(count)\n",
        "      else:\n",
        "        new_discarded.append(itemsets[s])\n",
        "  return L , supp_count, new_discarded\n",
        "\n",
        "def get_frequent_exact(itemsets, Transactions):\n",
        "  L = []\n",
        "  supp_count = []\n",
        "  new_discarded = []\n",
        "  num_trans = len(Transactions)\n",
        "\n",
        "  for s in range(len(itemsets)):\n",
        "    count = count_occurences(itemsets[s], Transactions)\n",
        "    \n",
        "    L.append(itemsets[s])\n",
        "    supp_count.append(count)\n",
        "  return L , supp_count\n",
        "\n",
        "def load_transactions(path_to_data, order):\n",
        "    Transactions = []\n",
        "    with open(path_to_data, 'r') as fid:\n",
        "      for lines in fid:\n",
        "        str_line = list(lines.strip().split(','))\n",
        "        _t = list(np.unique(str_line))\n",
        "        _t.sort(key = lambda x: order.index(x))\n",
        "        Transactions.append(_t)\n",
        "    return Transactions\n",
        "\n",
        "def print_PIS_table(T, supp_count, PIS):\n",
        "  print(\"Itemset|Frequency|PIS\")\n",
        "  for k in range(len(T)):\n",
        "    print(\"{} : {} : {}\".format(T[k], supp_count[k], PIS[k]))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_MIS_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "arr2 = []\n",
        "\n",
        "def print_Exact_table(T, supp_count,supp_count_L, Dict):\n",
        "  print(\"Exact Itemsets\")\n",
        "  print(\"-------------------------------------------\")\n",
        "  print(\"Itemset                | Frequency | MIS\")\n",
        "  count = 0\n",
        "  for k in range(len(T)):\n",
        "    arr = []\n",
        "    for i in range(len(T[k])):\n",
        "      for key, value in Dict.items():\n",
        "        if(T[k][i] == key):\n",
        "          arr.append(value)\n",
        "    if(supp_count[k] == min(arr)):\n",
        "      arr2.append(T[k])\n",
        "      print(\"{} :       {}    :   {}\".format(T[k], supp_count[k], min(arr)))\n",
        "      count += 1\n",
        "  if(count ==  0):\n",
        "        print(\"There is no exact itemsets\")\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def print_table(T, supp_count):\n",
        "    print(\"Itemset | Frequency\")\n",
        "    for k in range(len(T)):\n",
        "      print(\"{} : {}\".format(T[k], supp_count[k]))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "def join_two_itemsets(it1, it2, order):\n",
        "    it1.sort(key = lambda x: order.index(x))\n",
        "    it2.sort(key = lambda x: order.index(x))\n",
        "\n",
        "    for i in range(len(it1) - 1):\n",
        "      if it1[i] != it2[i]:\n",
        "        return []\n",
        "    if order.index(it1[-1]) < order.index(it2[-1]):\n",
        "      return it1 + [it2[-1]]\n",
        "\n",
        "    return []\n",
        "\n",
        "def join_set_itemsets(set_of_its, order):\n",
        "    C = []\n",
        "    for i in range(len(set_of_its)):\n",
        "      for j in range(i+1, len(set_of_its)):\n",
        "        it_out = join_two_itemsets(set_of_its[i], set_of_its[j], order)\n",
        "        if len(it_out) > 0:\n",
        "          C.append(it_out)\n",
        "    return C\n",
        "\n",
        "def powerset(s):\n",
        "  return list(chain.from_iterable(combinations(s,r) for r in range(1,len(s) + i)))\n",
        "\n",
        "\n",
        "def write_rules(X, X_S, S, conf, supp, num_trans):\n",
        "  out_rules = \"\"\n",
        "  out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "  out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "  out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "  out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  \n",
        "  return out_rules\n",
        "\n",
        "def write_rules_lift(X, X_S, S, conf, fre,num_trans):\n",
        "  out_rules = \"\"\n",
        "  lift = conf / fre\n",
        "  if(lift >= 0.25):\n",
        "    out_rules = \"\"\n",
        "    out_rules += \"Freq. Itemset: {}\\n\".format(X)\n",
        "    out_rules += \"     Rules: {} -> {} \\n\".format(list(S), list(X_S))\n",
        "    out_rules += \"     Conf: {0:2.3f} \\n\\n\".format(conf)\n",
        "    out_rules += \"     Lift: {0:2.3f} \\n\\n\".format(lift)\n",
        "    # out_rules += \"     Supp: {0:2.3f} \\n\\n\".format(supp / num_trans)\n",
        "  return out_rules\n",
        "\n",
        "Transactions = []\n",
        "filePath = input(\"Enter file path: \")\n",
        "with open(filePath, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "for i in range(len(data)):\n",
        "  Transactions.append(data[i])\n",
        "\n",
        "num_trans = len(Transactions)\n",
        "# Transactions\n",
        "# Transactions\n",
        "orders = []\n",
        "orders.append(Transactions[0][0])\n",
        "for i in range(0,len(Transactions)):\n",
        "  for j in range(len(Transactions[i])):\n",
        "    count = 0\n",
        "    k = 0\n",
        "    while(k < len(orders)):\n",
        "      if Transactions[i][j] == orders[k]:\n",
        "        count += 1\n",
        "        break\n",
        "      k += 1\n",
        "    if count == 0:\n",
        "      orders.append(Transactions[i][j])\n",
        "\n",
        "order = sorted(orders)\n",
        "C = {}\n",
        "L = {}\n",
        "itemset_size = 1\n",
        "num_trans = len(Transactions)\n",
        "supp_count_L = {}\n",
        "C.update({itemset_size : [ [f] for f in order]})\n",
        "\n",
        "assoc_rules_apriori_str = \"\"\n",
        "assoc_rules_apriori_str_lift = \"\"\n",
        "assoc_rules_exact_str = \"\"\n",
        "assoc_rules_apriori_str_exact_lift = \"\"\n",
        "min_support = float(input(\"Enter minimum support: \"))\n",
        "def aprioriAlgo(assoc_rules_apriori_str,assoc_rules_apriori_str_lift):\n",
        "    min_confidence = 1\n",
        "    Discarded = { itemset_size : []}\n",
        "    f, sup, new_discarded = get_frequent_apriori(C[itemset_size], Transactions, min_support, Discarded)\n",
        "    Discarded.update({itemset_size : new_discarded})\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})\n",
        "    print_table(L[1], supp_count_L[1])\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_table(C[k], [count_occurences(it, Transactions) for it in C[k]])\n",
        "      f,sup, new_discarded = get_frequent_apriori(C[k], Transactions, min_support,Discarded)\n",
        "      Discarded.update({k : new_discarded})\n",
        "      L.update({k : f})\n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 0:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print(\"Table L{}:\\n\".format(k))\n",
        "          print_table(L[k], supp_count_L[k])\n",
        "      k += 1\n",
        "    for i in range(1,len(L)):\n",
        "      for j in range(len(L[i])):\n",
        "        s = powerset(L[i][j])\n",
        "        s.pop()\n",
        "        for z in s:\n",
        "          S = set(z)\n",
        "          X = set(L[i][j])\n",
        "          X_S = set(X-S)\n",
        "          sup_x = count_occurences(X , Transactions)\n",
        "          sup_x_s = count_occurences(X_S, Transactions)\n",
        "          co = count_occurences(S, Transactions)\n",
        "          if co > 0:\n",
        "            conf = sup_x / co\n",
        "            if conf >= min_confidence:\n",
        "              assoc_rules_apriori_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "              assoc_rules_apriori_str_lift += write_rules_lift(X, X_S, S, conf, sup_x_s,num_trans)\n",
        "    return assoc_rules_apriori_str,assoc_rules_apriori_str_lift\n",
        "\n",
        "def exactRule(assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift):\n",
        "    assoc_rules_str = \"\"\n",
        "    min_conf = 1\n",
        "    PIS1 = []\n",
        "    PIS = []\n",
        "    Dict = {}\n",
        "    f1, sup1 = get_frequent_exact(C[itemset_size], Transactions)\n",
        "    max_PIS = min_support * num_trans\n",
        "\n",
        "    PISFilePath = input(\"Enter PIS file path: \")\n",
        "    f = open(PISFilePath, \"w\")\n",
        "    for i in range(0,len(order)):\n",
        "      f.write(str(random.randrange(0, max_PIS)) + \"\\n\")\n",
        "    f.close()\n",
        "\n",
        "    with open(PISFilePath, 'r') as file:\n",
        "      all_file = file.read().strip()  # Read and remove any extra new line\n",
        "      all_file_list = all_file.split('\\n')  # make a list of lines\n",
        "      PIS1 = [int(each_int) for each_int in all_file_list]\n",
        "    \n",
        "    f = []\n",
        "    sup = []\n",
        "    for i in range(0,len(PIS1)):\n",
        "      if(sup1[i] < PIS1[i]):\n",
        "        f.append(f1[i])\n",
        "        sup.append(sup1[i])\n",
        "        PIS.append(PIS1[i])\n",
        "    L.update({itemset_size : f})\n",
        "    supp_count_L.update({itemset_size : sup})  \n",
        "\n",
        "    # List is converting into string  \n",
        "    def convertList(list1):  \n",
        "    \n",
        "        return ' '.join([str(e) for e in list1]) #List comprehension  \n",
        "\n",
        "    for i in range(len(L[1])):\n",
        "        # case = {L[1][i] : supp_count_L[1][i]}\n",
        "        case = {convertList(L[1][i]) : supp_count_L[1][i]}\n",
        "        Dict.update(case)\n",
        "    # print(Dict)\n",
        "    print_PIS_table(L[1], supp_count_L[1], PIS)\n",
        "\n",
        "    k = itemset_size + 1\n",
        "    convergence = False\n",
        "    while not convergence:\n",
        "      arr = []\n",
        "      C.update({ k: join_set_itemsets(L[k-1], order)})\n",
        "      print(\"Table C{}:\\n\".format(k))\n",
        "      print_MIS_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      print_Exact_table(C[k], [count_occurences(it, Transactions) for it in C[k]], supp_count_L[1], Dict)\n",
        "      f,sup = get_frequent_exact(C[k], Transactions)\n",
        "      L.update({k : f})\n",
        "      supp_count_L.update({k : sup})\n",
        "      if len(L[k]) == 1 or len(L[k]) == 0:\n",
        "          convergence = True\n",
        "      else:\n",
        "          print()\n",
        "      k += 1\n",
        "\n",
        "    # for i in range(1, len(L)):\n",
        "    for i in range(len(arr2)):\n",
        "      # for j in range(len(arr2[i])):\n",
        "      s = list(powerset(arr2[i]))\n",
        "      s.pop()\n",
        "      for z in s:\n",
        "        S = set(z)\n",
        "        X = set(arr2[i])\n",
        "        X_S = set(X-S)\n",
        "        sup_x = count_occurences(X , Transactions)\n",
        "        sup_x_s = count_occurences(X_S, Transactions)\n",
        "        conf = sup_x / count_occurences(S, Transactions)\n",
        "        if conf >= min_conf:\n",
        "          assoc_rules_exact_str += write_rules(X, X_S, S, conf, sup_x,num_trans)\n",
        "          assoc_rules_apriori_str_exact_lift += write_rules_lift(X, X_S, S, conf, sup_x_s,num_trans)\n",
        "    return assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift\n",
        "\n",
        "print(\"***Rule mining using apriori algorithm***\")\n",
        "assoc_rules_apriori_str,assoc_rules_apriori_str_lift = aprioriAlgo(assoc_rules_apriori_str,assoc_rules_apriori_str_lift)\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_apriori_str)\n",
        "print(\"-----------------------Association Rule With Lift-------------------\\n\")\n",
        "print(assoc_rules_apriori_str_lift)\n",
        "print(\"***Rule mining using exact mining***\")\n",
        "assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift = exactRule(assoc_rules_exact_str,assoc_rules_apriori_str_exact_lift)\n",
        "print(\"-----------------------Association Rule-------------------\\n\")\n",
        "print(assoc_rules_exact_str)\n",
        "print(\"-----------------------Association Rule With Lift-------------------\\n\")\n",
        "print(assoc_rules_apriori_str_exact_lift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Putnam's work for software development\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "effort = int(input(\"Enter effort(developers) need to develop the software: \"))\n",
        "time = int(input(\"Enter estimated time for develop a software(in months): \"))\n",
        "# time = 0.3935 * effort\n",
        "# print(\"Estimated time for develop a software(in months): \" + str(time))\n",
        "tc = [2,8,11]\n",
        "\n",
        "def relay_graph():\n",
        "  print(\"\\n\")\n",
        "  \n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.xlabel(\"time\") \n",
        "  plt.ylabel(\"product size\")\n",
        "  plt.title(\"r_curve for putnam\")\n",
        "  plt.show()\n",
        "\n",
        "def putnam(tc,effort,time): \n",
        "  return tc * pow(effort, 1/3) * pow(time, 4/3)\n",
        "\n",
        "colors = {\n",
        "      2: 'red',\n",
        "      8: 'orange',\n",
        "      11: 'green'\n",
        "  }\n",
        "\n",
        "# for records in tc:\n",
        "#   points = []\n",
        "#   kloc = str(putnam(records,effort,time))\n",
        "#   project_duration = 3 * int(time)\n",
        "#   for timeTaken in range(0, project_duration):\n",
        "#     points.append(round(putnam(records, effort, timeTaken), 5))\n",
        "#   plt.plot(points, color=colors[records], linewidth=4, label=f\"TC: {records} | KLOC: {kloc}\")\n",
        "#   relay_graph()\n",
        "\n",
        "points = []\n",
        "k = []\n",
        "for records in tc:\n",
        "  a = []\n",
        "  kloc = str(putnam(records,effort,time))\n",
        "  project_duration = 3 * int(time)\n",
        "  for timeTaken in range(0, project_duration):\n",
        "    a.append(round(putnam(records, effort, timeTaken), 5))\n",
        "  \n",
        "  points.append(a)\n",
        "  k.append(kloc)\n",
        "  plt.plot(a, color=colors[records], linewidth=4, label=f\"TC: {records} | KLOC: {kloc}\")\n",
        "  relay_graph()\n",
        "i = 0\n",
        "for records in tc:\n",
        "  plt.plot(points[i], color=colors[records], linewidth=4, label=f\"TC: {records} | KLOC: {k[i]}\")\n",
        "  i += 1\n",
        "relay_graph()\n"
      ],
      "metadata": {
        "id": "u8sVMgaRIPTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Norden's Work for research and development\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "t = int(input(\"Enter effort time(t) in months: \"))\n",
        "K = int(input(\"Enter area under the curve: \"))\n",
        "Td = int(input(\"Enter the time at which the curve attains its maximum values(td) in months: \")) \n",
        "def norden(K,Td,t):\n",
        "  e = 2.7182\n",
        "  c1 = pow(Td,2)\n",
        "  c2 = pow(t,2)\n",
        "  c3 = -c2 / (2 * c1)\n",
        "  c4 = pow(e,c3)\n",
        "  return (K / c1) * t * c4\n",
        "def relayGraph(K,Td,t):\n",
        "  project_duration = 3 * int(Td)\n",
        "  y = []\n",
        "  for time in range(0, project_duration):\n",
        "    y.append(round(norden(K, Td, time), 5) )\n",
        "  plt.plot(y, color=\"blue\", linewidth=4, label=\"Effort respect to time\")\n",
        "  plt.vlines(\n",
        "        x=Td,\n",
        "        ymin=0,\n",
        "        ymax=round(norden(K, Td, Td), 5),\n",
        "        color=\"blue\",\n",
        "        linestyle=\"dashed\",\n",
        "        linewidth=4,\n",
        "        label=f\"td ({Td})\",\n",
        "    )\n",
        "  plt.vlines(\n",
        "        x=t,\n",
        "        ymin=0,\n",
        "        ymax=round(norden(K, Td, t), 5),\n",
        "        color=\"yellow\",\n",
        "        linestyle=\"dashed\",\n",
        "        linewidth=4,\n",
        "        label=\"t\"\n",
        "    )\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.xlabel(\"Time\")\n",
        "  plt.ylabel(\"Effort\")\n",
        "  plt.title(\"r_curve for norden\")\n",
        "  plt.show()\n",
        "print(f\"Effort(no of developers) required at time {t} months : {round(norden(K, Td, t), 5)}\")\n",
        "relayGraph(K,Td,t)"
      ],
      "metadata": {
        "id": "sDJnMbAYIJCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jensen's work for software development\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "effort = int(input(\"Enter effort(developers) need to develop the software: \"))\n",
        "time = int(input(\"Enter estimated time for develop a software(in months): \"))\n",
        "# time = 0.3935 * effort\n",
        "# print(\"Estimated time for develop a software(in months): \" + str(time))\n",
        "tc = [2,8,11]\n",
        "\n",
        "def relay_graph():\n",
        "  print(\"\\n\")\n",
        "  \n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.xlabel(\"time\") \n",
        "  plt.ylabel(\"product size\")\n",
        "  plt.title(\"r_curve for jensen\")\n",
        "  plt.show()\n",
        "\n",
        "def jensen(tc,effort,time): \n",
        "  return tc * time * pow(effort, 1/2)\n",
        "\n",
        "colors = {\n",
        "      2: 'red',\n",
        "      8: 'orange',\n",
        "      11: 'green'\n",
        "  }\n",
        "\n",
        "# for records in tc:\n",
        "#   points = []\n",
        "#   kloc = str(putnam(records,effort,time))\n",
        "#   project_duration = 3 * int(time)\n",
        "#   for timeTaken in range(0, project_duration):\n",
        "#     points.append(round(putnam(records, effort, timeTaken), 5))\n",
        "#   plt.plot(points, color=colors[records], linewidth=4, label=f\"TC: {records} | KLOC: {kloc}\")\n",
        "#   relay_graph()\n",
        "\n",
        "points = []\n",
        "k = []\n",
        "for records in tc:\n",
        "  a = []\n",
        "  kloc = str(jensen(records,effort,time))\n",
        "  project_duration = 3 * int(time)\n",
        "  for timeTaken in range(0, project_duration):\n",
        "    a.append(round(jensen(records, effort, timeTaken), 5))\n",
        "  \n",
        "  points.append(a)\n",
        "  k.append(kloc)\n",
        "  plt.plot(a, color=colors[records], linewidth=4, label=f\"TC: {records} | KLOC: {kloc}\")\n",
        "  relay_graph()\n",
        "i = 0\n",
        "for records in tc:\n",
        "  plt.plot(points[i], color=colors[records], linewidth=4, label=f\"TC: {records} | KLOC: {k[i]}\")\n",
        "  i += 1\n",
        "relay_graph()"
      ],
      "metadata": {
        "id": "i_5IlEqmIRGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7MDDSAlhIWX"
      },
      "source": [
        "# Basic COCOMO Model\n",
        "import matplotlib.pyplot as plt\n",
        "kloc = int(input(\"Enter total number of lines of code(KLOC): \"))\n",
        "\n",
        "colors = {\n",
        "      0: 'red',\n",
        "      1: 'orange',\n",
        "      2: 'green'\n",
        "  }\n",
        "effort = 0\n",
        "def relay_graph_effort(a,b,c,d,kloc,color):\n",
        "  effort,time = cocomo(a,b,c,d,kloc,0)\n",
        "  effortArr = []\n",
        "  klocArr = []\n",
        "  klocnew = kloc + 10\n",
        "  for i in range(1,kloc + 1):\n",
        "    effortArr.append(cocomo(a,b,c,d,i,1))\n",
        "    klocArr.append(i)\n",
        "  \n",
        "  plt.plot(klocArr,effortArr, color=colors[color], linewidth=4, label=f\"E: {effort} | T: {time}\")\n",
        "  draw_curve(1)\n",
        "  return effortArr\n",
        "  # print(effortArr)\n",
        "  # print(klocArr)\n",
        "\n",
        "def relay_graph_time(a,b,c,d,kloc,color):\n",
        "  effort,time = cocomo(a,b,c,d,kloc,3)\n",
        "  timeArr = []\n",
        "  klocArr = []\n",
        "  klocnew = kloc + 10\n",
        "  for i in range(1,kloc + 1):\n",
        "    timeArr.append(cocomo(a,b,c,d,i,2))\n",
        "    klocArr.append(i)\n",
        "  \n",
        "  plt.plot(klocArr,timeArr, color=colors[color], linewidth=4, label=f\"E: {effort} | T: {time}\")\n",
        "  draw_curve(0)\n",
        "  return timeArr\n",
        "\n",
        "def draw_curve(flag):\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.xlabel(\"KLOC\") \n",
        "  if flag == 1:\n",
        "    plt.ylabel(\"Estimated Effort\")\n",
        "    plt.title(\"Effort versus product size\")\n",
        "  else:\n",
        "    plt.ylabel(\"Estimated Time\")\n",
        "    plt.title(\"Time versus product size\")\n",
        "  plt.show()\n",
        "  print(\"\\n\")\n",
        "\n",
        "def cocomo(a,b,c,d,kloc,flag):\n",
        "  \n",
        "  effort = round((a * pow(kloc,b)), 4)\n",
        "  time = round((c * pow(effort,d)), 4)\n",
        "  personRequired = round((effort / time), 4)\n",
        "  productivity = round((kloc / effort),4)\n",
        "  if flag == 1:\n",
        "    return effort\n",
        "  if flag == 2:\n",
        "    return time\n",
        "  if flag == 3:\n",
        "    return effort,time\n",
        "  print(\"Effort: \" + str(effort)+ \" PM\")\n",
        "  print(\"Time: \" + str(time) + \" months\")\n",
        "  print(\"Average person required: \" + str(personRequired) + \" persons\")\n",
        "  print(\"Productivity: \" + str(productivity) + \" KLOC/PM\")\n",
        "  return effort,time\n",
        "\n",
        "effortArr = []\n",
        "klocArr = []\n",
        "timeArr = []\n",
        "for i in range(0,3):\n",
        "  if(i == 0):\n",
        "    print(\"In Organic Mode\")\n",
        "    effortArr.append(relay_graph_effort(2.4,1.05,2.5,0.38,kloc, 0))\n",
        "    timeArr.append(relay_graph_time(2.4,1.05,2.5,0.38,kloc, 0))\n",
        "  elif(i == 1):\n",
        "    print(\"In Semi Detached Mode\")\n",
        "    effortArr.append(relay_graph_effort(3.0\t,1.12\t,2.5,\t0.35, kloc, 1))\n",
        "    timeArr.append(relay_graph_time(3.0\t,1.12\t,2.5,\t0.35, kloc, 1))\n",
        "  elif(i== 2):\n",
        "    print(\"In Embedded Mode\")\n",
        "    effortArr.append(relay_graph_effort(3.6\t,1.20,\t2.5,\t0.32, kloc, 2))\n",
        "    timeArr.append(relay_graph_time(3.6\t,1.20,\t2.5,\t0.32, kloc, 2))\n",
        "for i in range(1,kloc + 1):\n",
        "    klocArr.append(i)\n",
        "\n",
        "for i in range(0,3):\n",
        "  plt.plot(klocArr,effortArr[i], color=colors[i], linewidth=4)\n",
        "  i += 1\n",
        "draw_curve(1)\n",
        "\n",
        "for i in range(0,3):\n",
        "  plt.plot(klocArr,timeArr[i], color=colors[i], linewidth=4)\n",
        "  i += 1\n",
        "draw_curve(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intermediate Cocomo Model \n"
      ],
      "metadata": {
        "id": "pJFCiRRcOg7B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7hqGyXBiMd8"
      },
      "source": [
        "# Intermediate Cocomo Model \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kloc = int(input(\"Enter total number of lines of code: \\n\"))\n",
        "constants = []\n",
        "\n",
        "colors = {\n",
        "      0: 'red',\n",
        "      1: 'orange',\n",
        "      2: 'green'\n",
        "  }\n",
        "effort = 0\n",
        "\n",
        "required_software_reliability = [0.75,0.88,1.00,1.15,1.40]\n",
        "size_of_application_database\t=\t[0,0.94,1.00,1.08,1.16]\n",
        "complexity_of_the_product =\t[0.70,0.85,1.00,1.15,1.30]\n",
        "runtime_performance_constraints = [0,0,1.00,\t1.11,\t1.30]\n",
        "memory_constraints = [0,0,1.00,1.06,1.21]\n",
        "volatility_of_the_virtual_machine_environment\t=\t[0,0.87,1.00,1.15,1.30]\n",
        "required_turnabout_time\t=\t[0,0.94,1.00,1.07,1.15]\n",
        "analyst_capability\t= [1.46,1.19,1.00,0.86,0.71]\n",
        "applications_experience\t= [1.29,1.13,1.00,0.91,0.82]\n",
        "software_engineer_capability\t= [1.42,\t1.17,\t1.00,\t0.86,\t0.70]\n",
        "virtual_machine_experience\t= [1.21,\t1.10,\t1.00,\t0.90, 0]\n",
        "programming_language_experience\t= [1.14,\t1.07,\t1.00,\t0.95, 0]\t\t\n",
        "application_of_software_engineering_methods =\t[1.24,\t1.10,\t1.00,\t0.91,\t0.82]\n",
        "use_of_s# Intermediate Cocomo Model oftware_tools =\t[1.24,\t1.10,\t1.00,\t0.91, 0.83]\n",
        "required_development_schedule\t= [1.23,\t1.08,\t1.00,\t1.04,\t1.10]\n",
        "\n",
        "informations = [\"Select Required Software Reliability: \",\n",
        "                \"Select Size of Application Database(Select between Low to Very High): \",\n",
        "                \"Select Complexity of The Product: \",\n",
        "                \"Select Runtime Performance Constraints(Select between Nominal to Very High): \",\n",
        "                \"Select Memory Constraints(Select between Nominal to Very High): \",\n",
        "                \"Select Volatility of the virtual machine environment(Select between Low to Very High): \",\n",
        "                \"Select Required turnabout time(Select between Low to Very High): \",\n",
        "                \"Select Analyst capability: \",\n",
        "                \"Select Applications experience: \",\n",
        "                \"Select Software engineer capability: \",\n",
        "                \"Select Virtual machine experience(Select between Very Low to High): \",\n",
        "        # Intermediate Cocomo Model         \"Select Programming language experience(Select between Very Low to High): \",\n",
        "                \"Select Application of software engineering methods: \",\n",
        "                \"Select Use of software tools: \",\n",
        "                \"Select Required development schedule: \"\n",
        "                ]\n",
        "\n",
        "def selectConstants(option,value):\n",
        "  if option == 0:\n",
        "      constants.append(required_software_reliability[value])\n",
        "  elif option == 1:\n",
        "      constants.append(size_of_application_database[value])\n",
        "  elif option == 2:\n",
        "      constants.append(complexity_of_the_product[value])\n",
        "  elif o# Intermediate Cocomo Model ption == 3:\n",
        "      constants.append(runtime_performance_constraints[value])\n",
        "  elif option == 4:\n",
        "      constants.append(memory_constraints[value])\n",
        "  elif option == 5:\n",
        "      constants.append(volatility_of_the_virtual_machine_environment[value])\n",
        "  elif option == 6:\n",
        "      constants.append(required_turnabout_time[value])\n",
        "  elif option == 7:\n",
        "      constants.append(analyst_capability[value])\n",
        "  elif option == 8:\n",
        "      constants.append(applications_experience[value])\n",
        "  elif option == 9:\n",
        "      constants.append(software_engineer_capability[value])\n",
        "  elif o# Intermediate Cocomo Model ption == 10:\n",
        "      constants.append(virtual_machine_experience[value])\n",
        "  elif option == 11:\n",
        "      constants.append(programming_language_experience[value])\n",
        "  elif option == 12:\n",
        "      constants.append(application_of_software_engineering_methods[value])\n",
        "  elif option == 13:\n",
        "      constants.append(use_of_software_tools[value])\n",
        "  elif option == 14:\n",
        "      constants.append(required_development_schedule[value])\n",
        "  \n",
        "def relay_graph_effort(a,b,kloc,color,EAF):\n",
        "  effort = cocomo(a,b,kloc,0,EAF)\n",
        "  effortArr = []\n",
        "  klocArr = []\n",
        "  klocnew = kloc + 10\n",
        "  for i in range(1,kloc + 1):\n",
        "    effortArr.append(cocomo(a,b,i,1,EAF))\n",
        "    klocArr.append(i)\n",
        "  \n",
        "  plt.plot(klocArr,effortArr, color=colors[color], linewidth=4, label=f\"E: {effort}\")\n",
        "  draw_curve(1)\n",
        "  return effortArr\n",
        "  # print(effortArr)\n",
        "  # print(klocArr)\n",
        "\n",
        "def draw_curve(flag):\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.xlabel(\"KLOC\") \n",
        "  if flag == 1:\n",
        "    plt.ylabel(\"Estimated Effort\")\n",
        "    plt.title(\"Effort versus product size\")\n",
        "  else:\n",
        "    plt.ylabel(\"Estimated Time\")\n",
        "    plt.title(\"Time versus product size\")\n",
        "  plt.show()\n",
        "  print(\"\\n\")\n",
        "\n",
        "def cocomo(a,b,kloc,flag,EAF):\n",
        "  effort = round((a * pow(kloc,b)), 4) * EAF\n",
        "  # time = round((c * pow(effort,d)), 4)\n",
        "  # personRequired = round((effort / time), 4)\n",
        "  # productivity = round((kloc / effort),4)\n",
        "  if flag == 1:\n",
        "    return effort\n",
        "  # if f# Intermediate Cocomo Model lag == 2:\n",
        "  #   return time\n",
        "  # if flag == 2:\n",
        "    # return effort,time\n",
        "  print(\"Effort: \" + str(effort)+ \" PM\")\n",
        "  # print(\"Time: \" + str(time) + \" months\")\n",
        "  # print(\"Average person required: \" + str(personRequired) + \" persons\")\n",
        "  # print(\"Productivity: \" + str(productivity) + \" KLOC/PM\")\n",
        "  # return effort,\n",
        "  return effort\n",
        "\n",
        "EAF  = 1\n",
        "for i in range(0,15):\n",
        "  \n",
        "  print(\"\\n\")\n",
        "  print(\"Very Low- Press 0\\nLow - Press 1\\nNominal - Press 2\\nHigh - Press 3\\nVery High - Press 4\")\n",
        "  flag = int(input(informations[i]))\n",
        "  select# Intermediate Cocomo Model Constants(i,flag)\n",
        "\n",
        "print(\"----------------------------------------------------------------------\\n\")\n",
        "for i in constants:\n",
        "      EAF = EAF * i\n",
        "\n",
        "print(\"Effort Adjustment Factor: \" + str(EAF) + \"\\n\")\n",
        "effortArr = []\n",
        "klocArr = []\n",
        "timeArr = []\n",
        "for i in range(0,3):\n",
        "  if(i == 0):\n",
        "    print(\"In Organic Mode\")\n",
        "    effortArr.append(relay_graph_effort(3.2,1.05,kloc, 0,EAF))\n",
        "    # timeArr.append(relay_graph_time(2.4,1.05,2.5,0.38,kloc, 0))\n",
        "  elif(i == 1):\n",
        "    print(\"In Semi Detached Mode\")\n",
        "    effortArr.append(relay_graph_effort(3.0\t,1.12, kloc, 1,EAF))\n",
        "    # timeArr.append(relay_graph_time(3.0\t,1.12\t,2.5,\t0.35, kloc, 1))\n",
        "  elif(i== 2):\n",
        "    print(\"In Embedded Mode\")\n",
        "    effortArr.append(relay_graph_effort(2.8\t,1.20, kloc, 2,EAF))\n",
        "    # timeArr.append(relay_graph_time(3.6\t,1.20,\t2.5,\t0.32, kloc, 2))\n",
        "for i in range(1,kloc + 1):\n",
        "    klocArr.append(i)\n",
        "\n",
        "for i in range(0,3):\n",
        "  plt.plot(klocArr,effortArr[i], color=colors[i], linewidth=4)\n",
        "  i += 1\n",
        "draw_curv\n",
        "  print(\"\\n\")\n",
        "  prie(1)\n",
        "\n",
        "# for i in range(0,3):\n",
        "#   plt.plot(klocArr,timeArr[i], color=colors[i], linewidth=4)\n",
        "#   i += 1\n",
        "# draw_curve(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detailed Cocomo Model "
      ],
      "metadata": {
        "id": "D_HZhFUyOlcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed Cocomo Model \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "kloc = int(input(\"Enter total number of lines of code: \\n\"))\n",
        "constants = []\n",
        "\n",
        "colors = {\n",
        "      0: 'red',\n",
        "      1: 'orange',\n",
        "      2: 'green'\n",
        "  }\n",
        "effort = 0\n",
        "\n",
        "required_software_reliability = [0.75,0.88,1.00,1.15,1.40]\n",
        "size_of_application_database\t=\t[0,0.94,1.00,1.08,1.16]\n",
        "complexity_of_the_product =\t[0.70,0.85,1.00,1.15,1.30]\n",
        "runtime_performance_constraints = [0,0,1.00,\t1.11,\t1.30]\n",
        "memory_constraints = [0,0,1.00,1.06,1.21]\n",
        "volatility_of_the_virtual_machine_environment\t=\t[0,0.87,1.00,1.15,1.30]\n",
        "required_turnabout_time\t=\t[0,0.94,1.00,1.07,1.15]\n",
        "analyst_capability\t= [1.46,1.19,1.00,0.86,0.71]\n",
        "applications_experience\t= [1.29,1.13,1.00,0.91,0.82]\n",
        "software_engineer_capability\t= [1.42,\t1.17,\t1.00,\t0.86,\t0.70]\n",
        "virtual_machine_experience\t= [1.21,\t1.10,\t1.00,\t0.90, 0]\n",
        "programming_language_experience\t= [1.14,\t1.07,\t1.00,\t0.95, 0]\t\t\n",
        "application_of_software_engineering_methods =\t[1.24,\t1.10,\t1.00,\t0.91,\t0.82]\n",
        "use_of_software_tools =\t[1.24,\t1.10,\t1.00,\t0.91, 0.83]\n",
        "required_development_schedule\t= [1.23,\t1.08,\t1.00,\t1.04,\t1.10]\n",
        "\n",
        "organic_small_up = [0.06,0.16,0.26,0.42,0.96]\n",
        "organic_mid_up = [0.06,0.16,0.24,0.38,0.22]\n",
        "s_det_mid_up = [0.07,0.17,0.25,0.33,0.25]\n",
        "s_det_large_up = [0.07,0.17,0.24,0.31,0.28]\n",
        "embaded_large_up = [0.08,0.18,0.25,0.26,0.31]\n",
        "embaded_xlarge_up = [0.08,0.18,0.24,0.24,0.34]\n",
        "\n",
        "organic_small_tp = [0.10,0.19,0.24,0.39,0.18]\n",
        "organic_mid_tp = [0.12,0.19,0.21,0.34,0.26]\n",
        "s_det_mid_tp = [0.20,0.26,0.21,0.27,0.26]\n",
        "s_det_large_tp = [0.22,0.27,0.19,0.25,0.29]\n",
        "embaded_large_tp = [0.36,0.36,0.18,0.18,0.28]\n",
        "embaded_xlarge_tp = [0.40,0.38,0.16,0.16,0.30]\n",
        "\n",
        "sofConst = [\"Plan & Requirement: \",\n",
        "            \"Design: \",\n",
        "            \"Detail Design: \",\n",
        "            \"Code & Test: \",\n",
        "            \"Integration & Test: \"\n",
        "            ]\n",
        "\n",
        "informations = [\"Select Required Software Reliability: \",\n",
        "                \"Select Size of Application Database(Select between Low to Very High): \",\n",
        "                \"Select Complexity of The Product: \",\n",
        "                \"Select Runtime Performance Constraints(Select between Nominal to Very High): \",\n",
        "                \"Select Memory Constraints(Select between Nominal to Very High): \",\n",
        "                \"Select Volatility of the virtual machine environment(Select between Low to Very High): \",\n",
        "                \"Select Required turnabout time(Select between Low to Very High): \",\n",
        "                \"Select Analyst capability: \",\n",
        "                \"Select Applications experience: \",\n",
        "                \"Select Software engineer capability: \",\n",
        "                \"Select Virtual machine experience(Select between Very Low to High): \",\n",
        "                \"Select Programming language experience(Select between Very Low to High): \",\n",
        "                \"Select Application of software engineering methods: \",\n",
        "                \"Select Use of software tools: \",\n",
        "                \"Select Required development schedule: \"\n",
        "                ]\n",
        "\n",
        "def selectConstants(option,value):\n",
        "  if option == 0:\n",
        "      constants.append(required_software_reliability[value])\n",
        "  elif option == 1:\n",
        "      constants.append(size_of_application_database[value])\n",
        "  elif option == 2:\n",
        "      constants.append(complexity_of_the_product[value])\n",
        "  elif option == 3:\n",
        "      constants.append(runtime_performance_constraints[value])\n",
        "  elif option == 4:\n",
        "      constants.append(memory_constraints[value])\n",
        "  elif option == 5:\n",
        "      constants.append(volatility_of_the_virtual_machine_environment[value])\n",
        "  elif option == 6:\n",
        "      constants.append(required_turnabout_time[value])\n",
        "  elif option == 7:\n",
        "      constants.append(analyst_capability[value])\n",
        "  elif option == 8:\n",
        "      constants.append(applications_experience[value])\n",
        "  elif option == 9:\n",
        "      constants.append(software_engineer_capability[value])\n",
        "  elif option == 10:\n",
        "      constants.append(virtual_machine_experience[value])\n",
        "  elif option == 11:\n",
        "      constants.append(programming_language_experience[value])\n",
        "  elif option == 12:\n",
        "      constants.append(application_of_software_engineering_methods[value])\n",
        "  elif option == 13:\n",
        "      constants.append(use_of_software_tools[value])\n",
        "  elif option == 14:\n",
        "      constants.append(required_development_schedule[value])\n",
        "  \n",
        "\n",
        "def detailedCocomo(E,T):\n",
        "  cons = []\n",
        "  # cons2 = []\n",
        "  cons3 = []\n",
        "  # cons4 = []\n",
        "  if(E < 32000):\n",
        "    for j in organic_small_up:\n",
        "      flag = round((j * E),4)\n",
        "      cons.append(flag)\n",
        "    for k in organic_small_tp:\n",
        "      flag = round((j * T),4)\n",
        "      cons3.append(flag)\n",
        "    return cons,cons3\n",
        "  elif(E >= 32000 and E < 128000):\n",
        "    for j in organic_mid_up:\n",
        "      flag = round((j * E),4)\n",
        "      cons.append(flag)\n",
        "    for k in organic_mid_tp:\n",
        "      flag = round((j * T),4)\n",
        "      cons3.append(flag)\n",
        "    return cons,cons3\n",
        "  elif(E >= 128000 and E < 320000):\n",
        "    for j in s_det_large_up:\n",
        "      flag = round((j * E),4)\n",
        "      cons.append(flag)\n",
        "    for k in s_det_large_tp:\n",
        "        flag = round((j * T),4)\n",
        "        cons3.append(flag)\n",
        "    return cons,cons3\n",
        "  elif(E >= 320000):\n",
        "    for j in embaded_xlarge_up:\n",
        "      flag = round((j * E),4)\n",
        "      cons.append(flag)\n",
        "    for k in embaded_xlarge_tp:\n",
        "      flag = round((j * T),4)\n",
        "      cons3.append(flag)\n",
        "    return cons,cons3\n",
        "\n",
        "def relay_graph(a,b,c,d,kloc,color,EAF):\n",
        "  cons = []\n",
        "  cons2 = []\n",
        "  effort,time = cocomo(a,b,c,d,kloc,0,EAF)\n",
        "  print(f\"Effort required {effort} PM\")\n",
        "  print(f\"Time required {time} months\")\n",
        "  cons,cons2 = detailedCocomo(effort,time)\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"Effort required in different phases\")\n",
        "  print(\"************************************************************\\n\")\n",
        "  for i in range(0,5):\n",
        "    print(sofConst[i] + str(cons[i]) + \" PM\")\n",
        "    \n",
        "  print(\"\\n\")\n",
        "  print(\"Development time in different phases\")\n",
        "  print(\"************************************************************\\n\")\n",
        "  for i in range(0,5):\n",
        "    print(sofConst[i] + str(cons2[i]) + \" Months\")\n",
        "  # effortArr = []\n",
        "  # effortPoints = []\n",
        "  # klocArr = []\n",
        "  # klocnew = kloc + 10\n",
        "  # l2 = []\n",
        "  # for i in range(1,kloc + 1):\n",
        "  #   pointEffort = cocomo(a,b,c,d,i,1,EAF)\n",
        "  #   effortPoints.append(detailedCocomo(pointEffort,time))\n",
        "  #   klocArr.append(i)\n",
        "  \n",
        "  # l2 = transpose(effortPoints, l2)\n",
        "  # for i in range(0,5):\n",
        "  #   plt.plot(klocArr,l2[i], color=colors[color], linewidth=4, label=f\"E: {effort}\")\n",
        "  # draw_curve(1)\n",
        "  # return effortArr\n",
        "\n",
        "def transpose(l1, l2):\n",
        " \n",
        "    l2 = list(map(list, zip(*l1)))\n",
        "    return l2\n",
        "\n",
        "\n",
        "def draw_curve(flag):\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.xlabel(\"KLOC\") \n",
        "  if flag == 1:\n",
        "    plt.ylabel(\"Estimated Effort\")\n",
        "    plt.title(\"Effort versus product size\")\n",
        "  else:\n",
        "    plt.ylabel(\"Estimated Time\")\n",
        "    plt.title(\"Time versus product size\")\n",
        "  plt.show()\n",
        "  print(\"\\n\")\n",
        "\n",
        "def cocomo(a,b,c,d,kloc,flag,EAF):\n",
        "  effort = round((a * pow(kloc,b)), 4) * EAF\n",
        "  time = round((c * pow(effort,d)), 4)\n",
        "  # personRequired = round((effort / time), 4)\n",
        "  # productivity = round((kloc / effort),4)\n",
        "  if flag == 1:\n",
        "    return effort\n",
        "  if flag == 2:\n",
        "    return time\n",
        "  # print(\"Effort: \" + str(effort)+ \" PM\")\n",
        "  # print(\"Time: \" + str(time) + \" months\")\n",
        "  # print(\"Average person required: \" + str(personRequired) + \" persons\")\n",
        "  # print(\"Productivity: \" + str(productivity) + \" KLOC/PM\")\n",
        "  # return effort,\n",
        "  return effort,time\n",
        "\n",
        "EAF  = 1\n",
        "for i in range(0,15):\n",
        "  \n",
        "  print(\"\\n\")\n",
        "  print(\"Very Low- Press 0\\nLow - Press 1\\nNominal - Press 2\\nHigh - Press 3\\nVery High - Press 4\")\n",
        "  flag = int(input(informations[i]))\n",
        "  selectConstants(i,flag)\n",
        "\n",
        "print(\"----------------------------------------------------------------------\\n\")\n",
        "for i in constants:\n",
        "      EAF = EAF * i\n",
        "\n",
        "print(\"Effort Adjustment Factor: \" + str(EAF) + \"\\n\")\n",
        "effortArr = []\n",
        "klocArr = []\n",
        "timeArr = []\n",
        "for i in range(0,3):\n",
        "  if(i == 0):\n",
        "    print(\"In Organic Mode\")\n",
        "    print(\"--------------------\\n\")\n",
        "    effortArr.append(relay_graph(2.4,1.05,2.5,0.38,kloc, 0,EAF))\n",
        "    print(\"\\n\")\n",
        "    # timeArr.append(relay_graph_time(2.4,1.05,2.5,0.38,kloc, 0))\n",
        "  elif(i == 1):\n",
        "    print(\"In Semi Detached Mode\")\n",
        "    print(\"--------------------\\n\")\n",
        "    effortArr.append(relay_graph(3.0,1.12,2.5,0.35, kloc, 1,EAF))\n",
        "    print(\"\\n\")\n",
        "    # timeArr.append(relay_graph_time(3.0\t,1.12\t,2.5,\t0.35, kloc, 1))\n",
        "  elif(i== 2):\n",
        "    print(\"In Embedded Mode\")\n",
        "    print(\"--------------------\\n\")\n",
        "    effortArr.append(relay_graph(3.6,1.20,2.5,0.32, kloc, 2,EAF))\n",
        "    print(\"\\n\")\n",
        "    # timeArr.append(relay_graph_time(3.6\t,1.20,\t2.5,\t0.32, kloc, 2))\n",
        "for i in range(1,kloc + 1):\n",
        "    klocArr.append(i)\n",
        "\n",
        "# for i in range(0,3):\n",
        "#   plt.plot(klocArr,effortArr[i], color=colors[i], linewidth=4)\n",
        "#   i += 1\n",
        "# draw_curve(1)\n",
        "\n",
        "# for i in range(0,3):\n",
        "#   plt.plot(klocArr,timeArr[i], color=colors[i], linewidth=4)\n",
        "#   i += 1\n",
        "# draw_curve(0)"
      ],
      "metadata": {
        "id": "Q97FtuTQToMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6b10f4-cb3b-4017-c493-8817a1550cd6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter total number of lines of code: \n",
            "12000\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Required Software Reliability: 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Size of Application Database(Select between Low to Very High): 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Complexity of The Product: 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Runtime Performance Constraints(Select between Nominal to Very High): 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Memory Constraints(Select between Nominal to Very High): 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Volatility of the virtual machine environment(Select between Low to Very High): 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Required turnabout time(Select between Low to Very High): 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Analyst capability: 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Applications experience: 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Software engineer capability: 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Virtual machine experience(Select between Very Low to High): 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Programming language experience(Select between Very Low to High): 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Application of software engineering methods: 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Use of software tools: 2\n",
            "\n",
            "\n",
            "Very Low- Press 0\n",
            "Low - Press 1\n",
            "Nominal - Press 2\n",
            "High - Press 3\n",
            "Very High - Press 4\n",
            "Select Required development schedule: 2\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Effort Adjustment Factor: 1.0\n",
            "\n",
            "In Organic Mode\n",
            "--------------------\n",
            "\n",
            "Effort required 46062.929 PM\n",
            "Time required 147.9155 months\n",
            "\n",
            "\n",
            "Effort required in different phases\n",
            "************************************************************\n",
            "\n",
            "Plan & Requirement: 2763.7757 PM\n",
            "Design: 7370.0686 PM\n",
            "Detail Design: 11055.103 PM\n",
            "Code & Test: 17503.913 PM\n",
            "Integration & Test: 10133.8444 PM\n",
            "\n",
            "\n",
            "Development time in different phases\n",
            "************************************************************\n",
            "\n",
            "Plan & Requirement: 32.5414 Months\n",
            "Design: 32.5414 Months\n",
            "Detail Design: 32.5414 Months\n",
            "Code & Test: 32.5414 Months\n",
            "Integration & Test: 32.5414 Months\n",
            "\n",
            "\n",
            "In Semi Detached Mode\n",
            "--------------------\n",
            "\n",
            "Effort required 111123.0749 PM\n",
            "Time required 145.8719 months\n",
            "\n",
            "\n",
            "Effort required in different phases\n",
            "************************************************************\n",
            "\n",
            "Plan & Requirement: 6667.3845 PM\n",
            "Design: 17779.692 PM\n",
            "Detail Design: 26669.538 PM\n",
            "Code & Test: 42226.7685 PM\n",
            "Integration & Test: 24447.0765 PM\n",
            "\n",
            "\n",
            "Development time in different phases\n",
            "************************************************************\n",
            "\n",
            "Plan & Requirement: 32.0918 Months\n",
            "Design: 32.0918 Months\n",
            "Detail Design: 32.0918 Months\n",
            "Code & Test: 32.0918 Months\n",
            "Integration & Test: 32.0918 Months\n",
            "\n",
            "\n",
            "In Embedded Mode\n",
            "--------------------\n",
            "\n",
            "Effort required 282696.2165 PM\n",
            "Time required 138.7914 months\n",
            "\n",
            "\n",
            "Effort required in different phases\n",
            "************************************************************\n",
            "\n",
            "Plan & Requirement: 19788.7352 PM\n",
            "Design: 48058.3568 PM\n",
            "Detail Design: 67847.092 PM\n",
            "Code & Test: 87635.8271 PM\n",
            "Integration & Test: 79154.9406 PM\n",
            "\n",
            "\n",
            "Development time in different phases\n",
            "************************************************************\n",
            "\n",
            "Plan & Requirement: 38.8616 Months\n",
            "Design: 38.8616 Months\n",
            "Detail Design: 38.8616 Months\n",
            "Code & Test: 38.8616 Months\n",
            "Integration & Test: 38.8616 Months\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UDM7889cJPGC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}